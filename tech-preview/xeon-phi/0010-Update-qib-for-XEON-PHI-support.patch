IB/qib: Update qib for XEON PHI support

From: Jubin John <jubin.john@intel.com>

Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
Signed-off-by: Jubin John <jubin.john@intel.com>
---
diff -ruN a9/drivers/infiniband/hw/qib/Makefile a10/drivers/infiniband/hw/qib/Makefile
--- a9/drivers/infiniband/hw/qib/Makefile	2015-09-10 09:35:36.420958201 -0700
+++ a10/drivers/infiniband/hw/qib/Makefile	2015-09-10 09:36:03.135901227 -0700
@@ -14,3 +14,8 @@
 ib_qib-$(CONFIG_X86_64) += qib_wc_x86_64.o
 ib_qib-$(CONFIG_PPC64) += qib_wc_ppc64.o
 ib_qib-$(CONFIG_DEBUG_FS) += qib_debugfs.o
+
+ifeq ($(CONFIG_INFINIBAND_SCIF),m)
+ib_qib-y += qib_knx.o
+ccflags-y += -DQIB_CONFIG_KNX
+endif
diff -ruN a9/drivers/infiniband/hw/qib/qib_common.h a10/drivers/infiniband/hw/qib/qib_common.h
--- a9/drivers/infiniband/hw/qib/qib_common.h	2015-09-10 09:35:36.420958201 -0700
+++ a10/drivers/infiniband/hw/qib/qib_common.h	2015-09-10 09:36:03.136900809 -0700
@@ -1,4 +1,5 @@
 /*
+ * Copyright (c) 2012 Intel Corporation. All rights reserved.
  * Copyright (c) 2006, 2007, 2008, 2009, 2010 QLogic Corporation.
  * All rights reserved.
  * Copyright (c) 2003, 2004, 2005, 2006 PathScale, Inc. All rights reserved.
@@ -337,8 +338,12 @@
 	 * Should be set to QIB_USER_SWVERSION.
 	 */
 	__u32 spu_userversion;
-
+#ifdef QIB_CONFIG_KNX
+	__u16 spu_knx_node_id;
+	__u16 _spu_unused2;
+#else
 	__u32 _spu_unused2;
+#endif
 
 	/* size of struct base_info to write to */
 	__u32 spu_base_info_size;
diff -ruN a9/drivers/infiniband/hw/qib/qib_file_ops.c a10/drivers/infiniband/hw/qib/qib_file_ops.c
--- a9/drivers/infiniband/hw/qib/qib_file_ops.c	2015-09-10 09:35:36.418958234 -0700
+++ a10/drivers/infiniband/hw/qib/qib_file_ops.c	2015-09-10 09:36:03.138932454 -0700
@@ -48,6 +48,7 @@
 #include "qib.h"
 #include "qib_common.h"
 #include "qib_user_sdma.h"
+#include "qib_knx.h"
 
 #undef pr_fmt
 #define pr_fmt(fmt) QIB_DRV_NAME ": " fmt
@@ -59,6 +60,9 @@
 			     unsigned long, loff_t);
 static unsigned int qib_poll(struct file *, struct poll_table_struct *);
 static int qib_mmapf(struct file *, struct vm_area_struct *);
+static int subctxt_search_ctxts(struct qib_devdata *, struct file *,
+				const struct qib_user_info *);
+
 
 static const struct file_operations qib_file_ops = {
 	.owner = THIS_MODULE,
@@ -89,6 +93,64 @@
 	return paddr;
 }
 
+#ifdef QIB_CONFIG_KNX
+/*
+ * Fills in only a few of the fields in the qib_base_info structure so the
+ * module on the KNX size can allocate all necessary memories locally.
+ */
+static int qib_get_early_base_info(struct file *fp, void __user *ubase,
+				   size_t ubase_size) {
+	struct qib_ctxtdata *rcd = ctxt_fp(fp);
+	int ret = 0;
+	struct qib_devdata *dd = rcd->dd;
+	struct qib_base_info *kinfo = NULL;
+	size_t sz;
+	int local_node = (numa_node_id() == pcibus_to_node(dd->pcidev->bus));
+
+	sz = sizeof(*kinfo);
+	if (!rcd->subctxt_cnt)
+		sz -= 7 * sizeof(u64);
+	if (ubase_size < sz) {
+		ret = -EINVAL;
+		goto bail;
+	}
+
+	kinfo = kzalloc(sizeof(*kinfo), GFP_KERNEL);
+	if (kinfo == NULL) {
+		ret = -ENOMEM;
+		goto bail;
+	}
+
+	ret = dd->f_get_base_info(rcd, kinfo);
+	if (ret < 0)
+		goto bail_free;
+
+	if (rcd->subctxt_cnt && !subctxt_fp(fp))
+		kinfo->spi_runtime_flags |= QIB_RUNTIME_MASTER;
+
+	kinfo->spi_unit = dd->unit;
+	kinfo->spi_port = rcd->ppd->port;
+	kinfo->spi_ctxt = rcd->ctxt;
+	kinfo->spi_subctxt = subctxt_fp(fp);
+	kinfo->spi_rcvhdr_cnt = dd->rcvhdrcnt;
+	kinfo->spi_rcvhdrent_size = dd->rcvhdrentsize;
+	kinfo->spi_rcv_egrbufsize = dd->rcvegrbufsize;
+	kinfo->spi_rcv_egrbuftotlen =
+		rcd->rcvegrbuf_chunks * rcd->rcvegrbuf_size;
+	kinfo->spi_rcv_egrperchunk = rcd->rcvegrbufs_perchunk;
+	kinfo->spi_rcv_egrchunksize = kinfo->spi_rcv_egrbuftotlen /
+		rcd->rcvegrbuf_chunks;
+
+	sz = (ubase_size < sizeof(*kinfo)) ? ubase_size : sizeof(*kinfo);
+	if (copy_to_user(ubase, kinfo, sz))
+		ret = -EFAULT;
+bail_free:
+	kfree(kinfo);
+bail:
+	return ret;
+}
+#endif
+
 static int qib_get_base_info(struct file *fp, void __user *ubase,
 			     size_t ubase_size)
 {
@@ -177,14 +239,43 @@
 	 */
 	kinfo->spi_rcvhdr_base = (u64) rcd->rcvhdrq_phys;
 	kinfo->spi_rcvhdr_tailaddr = (u64) rcd->rcvhdrqtailaddr_phys;
+	/*
+	 * In the case of KNX, qib_do_user_init() would call into the
+	 * KNX-specific memory allocation/registration functions. These
+	 * functions will write the registered memory offsets in the
+	 * qib_base_info structure. Those are the addresses that need to be
+	 * handled to user level.
+	 */
+	kinfo->spi_uregbase = knx_node_fp(fp) ?
+		qib_knx_ctxt_info(rcd, QIB_KNX_CTXTINFO_UREG, fp) :
+		(u64) dd->uregbase + dd->ureg_align * rcd->ctxt;
+
+	if (knx_node_fp(fp))
+		kinfo->spi_runtime_flags =
+			qib_knx_ctxt_info(rcd, QIB_KNX_CTXTINFO_FLAGS, fp);
 	kinfo->spi_rhf_offset = dd->rhf_offset;
 	kinfo->spi_rcv_egrbufs = (u64) rcd->rcvegr_phys;
-	kinfo->spi_pioavailaddr = (u64) dd->pioavailregs_phys;
+
+	/* see comment for spi_uregbase above */
+	if (knx_node_fp(fp))
+		kinfo->spi_pioavailaddr =
+			qib_knx_ctxt_info(rcd, QIB_KNX_CTXTINFO_PIOAVAIL, fp);
+	else
+		kinfo->spi_pioavailaddr = (u64) dd->pioavailregs_phys;
+
 	/* setup per-unit (not port) status area for user programs */
-	kinfo->spi_status = (u64) kinfo->spi_pioavailaddr +
-		(char *) ppd->statusp -
-		(char *) dd->pioavailregs_dma;
-	kinfo->spi_uregbase = (u64) dd->uregbase + dd->ureg_align * rcd->ctxt;
+	kinfo->spi_status = (knx_node_fp(fp) ?
+			     qib_knx_ctxt_info(
+				     rcd, QIB_KNX_CTXTINFO_STATUS, fp) :
+			     (u64) dd->pioavailregs_phys) +
+		(char *) ppd->statusp -	(char *) dd->pioavailregs_dma;
+
+	/*
+	 * Do not set spi_piobufbase to KNX offset here as it is used in
+	 * PIO index calculations below. For KNX contexts, the value of
+	 * spi_piobufbase is not the physical address but the offset of
+	 * the registered memory.
+	 */
 	if (!shared) {
 		kinfo->spi_piocnt = rcd->piocnt;
 		kinfo->spi_piobufbase = (u64) rcd->piobufs;
@@ -204,7 +295,11 @@
 			dd->palign * kinfo->spi_piocnt * slave;
 	}
 
-	if (shared) {
+	/*
+	 * In the case of KNX contexts, shared context memory is setup and
+	 * handled on the the KNX.
+	 */
+	if (shared && !knx_node_fp(fp)) {
 		kinfo->spi_sendbuf_status =
 			cvt_kvaddr(&rcd->user_event_mask[subctxt_fp(fp)]);
 		/* only spi_subctxt_* fields should be set in this block! */
@@ -225,6 +320,11 @@
 	kinfo->spi_pioindex = (kinfo->spi_piobufbase - dd->pio2k_bufbase) /
 		dd->palign;
 	kinfo->spi_pioalign = dd->palign;
+	/* Update spi_piobufbase after all calculations are done. */
+	if (knx_node_fp(fp))
+		kinfo->spi_piobufbase =
+			qib_knx_ctxt_info(rcd, QIB_KNX_CTXTINFO_PIOBUFBASE, fp);
+
 	kinfo->spi_qpair = QIB_KD_QP;
 	/*
 	 * user mode PIO buffers are always 2KB, even when 4KB can
@@ -1261,6 +1361,17 @@
 		goto bail;
 	}
 
+#ifdef QIB_CONFIG_KNX
+	if (uinfo->spu_knx_node_id)
+		/*
+		 * When setting up a context for a KNX process, setup of
+		 * the subcontexts memory is done on the KNX side and
+		 * mapped into user level. Therefore, the host driver never
+		 * has to worry about it unless we are setting up a context
+		 * on the host.
+		 */
+		goto no_subctxt_mem;
+#endif
 	rcd->subctxt_uregbase = vmalloc_user(PAGE_SIZE * num_subctxts);
 	if (!rcd->subctxt_uregbase) {
 		ret = -ENOMEM;
@@ -1283,6 +1394,9 @@
 		goto bail_rhdr;
 	}
 
+#ifdef QIB_CONFIG_KNX
+no_subctxt_mem:
+#endif
 	rcd->subctxt_cnt = uinfo->spu_subctxt_cnt;
 	rcd->subctxt_id = uinfo->spu_subctxt_id;
 	rcd->active_slaves = 1;
@@ -1317,6 +1431,14 @@
 
 	rcd = qib_create_ctxtdata(ppd, ctxt, numa_id);
 
+#ifdef QIB_CONFIG_KNX
+	if (uinfo->spu_knx_node_id)
+		/*
+		 * Skip allocation of page pointer list for TID
+		 * receives. This will be done on the KNX.
+		 */
+		goto no_page_list;
+#endif
 	/*
 	 * Allocate memory for use in qib_tid_update() at open to
 	 * reduce cost of expected send setup per message segment
@@ -1332,7 +1454,11 @@
 		ret = -ENOMEM;
 		goto bailerr;
 	}
+#ifdef QIB_CONFIG_KNX
+no_page_list:
+#endif
 	rcd->userversion = uinfo->spu_userversion;
+
 	ret = init_subctxts(dd, rcd, uinfo);
 	if (ret)
 		goto bailerr;
@@ -1489,43 +1615,68 @@
 static int find_shared_ctxt(struct file *fp,
 			    const struct qib_user_info *uinfo)
 {
-	int devmax, ndev, i;
+	int devmax, ndev;
 	int ret = 0;
+	struct qib_devdata *dd;
 
+#ifdef QIB_CONFIG_KNX
+	/*
+	 * In the case we are allocating a context for a KNX process,
+	 * Don't loop over all devices but use the one assosiated with the
+	 * requesting KNX.
+	 */
+	if (uinfo->spu_knx_node_id) {
+		dd = qib_knx_node_to_dd(uinfo->spu_knx_node_id);
+		if (dd && dd->num_knx)
+			ret = subctxt_search_ctxts(dd, fp, uinfo);
+		goto done;
+	}
+#endif
 	devmax = qib_count_units(NULL, NULL);
 
 	for (ndev = 0; ndev < devmax; ndev++) {
-		struct qib_devdata *dd = qib_lookup(ndev);
-
+		dd = qib_lookup(ndev);
 		/* device portion of usable() */
 		if (!(dd && (dd->flags & QIB_PRESENT) && dd->kregbase))
 			continue;
-		for (i = dd->first_user_ctxt; i < dd->cfgctxts; i++) {
-			struct qib_ctxtdata *rcd = dd->rcd[i];
+		ret = subctxt_search_ctxts(dd, fp, uinfo);
+		if (ret)
+			break;
+	}
+#ifdef QIB_CONFIG_KNX
+done:
+#endif
+	return ret;
+}
 
-			/* Skip ctxts which are not yet open */
-			if (!rcd || !rcd->cnt)
-				continue;
-			/* Skip ctxt if it doesn't match the requested one */
-			if (rcd->subctxt_id != uinfo->spu_subctxt_id)
-				continue;
-			/* Verify the sharing process matches the master */
-			if (rcd->subctxt_cnt != uinfo->spu_subctxt_cnt ||
-			    rcd->userversion != uinfo->spu_userversion ||
-			    rcd->cnt >= rcd->subctxt_cnt) {
-				ret = -EINVAL;
-				goto done;
-			}
-			ctxt_fp(fp) = rcd;
-			subctxt_fp(fp) = rcd->cnt++;
-			rcd->subpid[subctxt_fp(fp)] = current->pid;
-			tidcursor_fp(fp) = 0;
-			rcd->active_slaves |= 1 << subctxt_fp(fp);
-			ret = 1;
+static int subctxt_search_ctxts(struct qib_devdata *dd, struct file *fp,
+				const struct qib_user_info *uinfo)
+{
+	int ret = 0, i;
+	for (i = dd->first_user_ctxt; i < dd->cfgctxts; i++) {
+		struct qib_ctxtdata *rcd = dd->rcd[i];
+
+		/* Skip ctxts which are not yet open */
+		if (!rcd || !rcd->cnt)
+			continue;
+		/* Skip ctxt if it doesn't match the requested one */
+		if (rcd->subctxt_id != uinfo->spu_subctxt_id)
+			continue;
+		/* Verify the sharing process matches the master */
+		if (rcd->subctxt_cnt != uinfo->spu_subctxt_cnt ||
+		    rcd->userversion != uinfo->spu_userversion ||
+		    rcd->cnt >= rcd->subctxt_cnt) {
+			ret = -EINVAL;
 			goto done;
 		}
+		ctxt_fp(fp) = rcd;
+		subctxt_fp(fp) = rcd->cnt++;
+		rcd->subpid[subctxt_fp(fp)] = current->pid;
+		tidcursor_fp(fp) = 0;
+		rcd->active_slaves |= 1 << subctxt_fp(fp);
+		ret = 1;
+		break;
 	}
-
 done:
 	return ret;
 }
@@ -1617,6 +1768,13 @@
 
 	if (swminor >= 11 && uinfo->spu_port_alg < QIB_PORT_ALG_COUNT)
 		alg = uinfo->spu_port_alg;
+#ifdef QIB_CONFIG_KNX
+	/* Make sure we have a connection to the KNX module on the right node */
+	if (uinfo->spu_knx_node_id && !qib_knx_get(uinfo->spu_knx_node_id)) {
+		ret = -ENODEV;
+		goto done;
+	}
+#endif
 
 	mutex_lock(&qib_mutex);
 
@@ -1624,13 +1782,38 @@
 	    uinfo->spu_subctxt_cnt) {
 		ret = find_shared_ctxt(fp, uinfo);
 		if (ret > 0) {
-			ret = do_qib_user_sdma_queue_create(fp);
+#ifdef QIB_CONFIG_KNX
+			if (uinfo->spu_knx_node_id) {
+				ret = qib_knx_sdma_queue_create(fp);
+			} else
+#endif
+				ret = do_qib_user_sdma_queue_create(fp);
 			if (!ret)
 				assign_ctxt_affinity(fp, (ctxt_fp(fp))->dd);
 			goto done_ok;
 		}
 	}
 
+#ifdef QIB_CONFIG_KNX
+	/*
+	 * If there is a KNX node set, we pick the device that is
+	 * associate with that KNX node
+	 */
+	if (uinfo->spu_knx_node_id) {
+		struct qib_devdata *dd =
+			qib_knx_node_to_dd(uinfo->spu_knx_node_id);
+		if (dd) {
+			ret = find_free_ctxt(dd->unit, fp, uinfo);
+			if (!ret)
+				ret = qib_knx_alloc_ctxt(
+					uinfo->spu_knx_node_id,
+					ctxt_fp(fp)->ctxt);
+		} else
+			ret = -ENXIO;
+		goto done_chk_sdma;
+	}
+
+#endif
 	i_minor = iminor(file_inode(fp)) - QIB_USER_MINOR_BASE;
 	if (i_minor)
 		ret = find_free_ctxt(i_minor - 1, fp, uinfo);
@@ -1639,7 +1822,6 @@
 		const unsigned int cpu = cpumask_first(&current->cpus_allowed);
 		const unsigned int weight =
 			cpumask_weight(&current->cpus_allowed);
-
 		if (weight == 1 && !test_bit(cpu, qib_cpulist))
 			if (!find_hca(cpu, &unit) && unit >= 0)
 				if (!find_free_ctxt(unit, fp, uinfo)) {
@@ -1650,9 +1832,21 @@
 	}
 
 done_chk_sdma:
-	if (!ret)
+	if (!ret) {
+#ifdef QIB_CONFIG_KNX
+		if (uinfo->spu_knx_node_id) {
+			ret = qib_knx_sdma_queue_create(fp);
+			/*if (!ret)
+			  ret = qib_knx_setup_tidrcv(fp);*/
+			goto done_ok;
+		}
+#endif
 		ret = do_qib_user_sdma_queue_create(fp);
+	}
 done_ok:
+#ifdef QIB_CONFIG_KNX
+	knx_node_fp(fp) = uinfo->spu_knx_node_id;
+#endif
 	mutex_unlock(&qib_mutex);
 
 done:
@@ -1667,11 +1861,25 @@
 	struct qib_ctxtdata *rcd = ctxt_fp(fp);
 	struct qib_devdata *dd;
 	unsigned uctxt;
+#ifdef QIB_CONFIG_KNX
+	struct qib_base_info *base_info = NULL;
+	void __user *ubase = (void __user *)(unsigned long)
+		uinfo->spu_base_info;
+#endif
 
 	/* Subctxts don't need to initialize anything since master did it. */
 	if (subctxt_fp(fp)) {
 		ret = wait_event_interruptible(rcd->wait,
 			!test_bit(QIB_CTXT_MASTER_UNINIT, &rcd->flag));
+#ifdef QIB_CONFIG_KNX
+		/*
+		 * Subctxt pio buffers need to be registered after the
+		 * master has set everything up.
+		 */
+		if (uinfo->spu_knx_node_id)
+			ret = qib_knx_setup_piobufs(rcd->dd, rcd,
+						    subctxt_fp(fp));
+#endif
 		goto bail;
 	}
 
@@ -1722,6 +1930,41 @@
 	 */
 	dd->f_sendctrl(dd->pport, QIB_SENDCTRL_AVAIL_BLIP);
 
+#ifdef QIB_CONFIG_KNX
+	if (uinfo->spu_knx_node_id) {
+		/*
+		 * When setting up rcvhdr Q and eager buffers for a KNX, the
+		 * memory comes from the KNX side encoded in the qib_base_info
+		 * structure.
+		 */
+		if (uinfo->spu_base_info_size < (sizeof(*base_info) -
+						 7 * sizeof(u64))) {
+			ret = -EINVAL;
+			goto bail_pio;
+		}
+		base_info = kzalloc(sizeof(*base_info), GFP_KERNEL);
+		if (!base_info) {
+			ret = -ENOMEM;
+			goto bail_pio;
+		}
+		if (copy_from_user(base_info, ubase,
+				   uinfo->spu_base_info_size)) {
+			ret = -EFAULT;
+			goto bail_pio;
+		}
+		ret = qib_knx_setup_piobufs(dd, rcd, subctxt_fp(fp));
+		if (ret)
+			goto cont_init;
+		ret = qib_knx_setup_pioregs(dd, rcd, base_info);
+		if (ret)
+			goto cont_init;
+		ret = qib_knx_create_rcvhdrq(dd, rcd, base_info);
+		if (ret)
+			goto cont_init;
+		ret = qib_knx_setup_eagerbufs(rcd, base_info);
+		goto cont_init;
+	}
+#endif /* QIB_CONFIG_KNX */
 	/*
 	 * Now allocate the rcvhdr Q and eager TIDs; skip the TID
 	 * array for time being.  If rcd->ctxt > chip-supported,
@@ -1731,6 +1974,9 @@
 	ret = qib_create_rcvhdrq(dd, rcd);
 	if (!ret)
 		ret = qib_setup_eagerbufs(rcd);
+#ifdef QIB_CONFIG_KNX
+cont_init:
+#endif
 	if (ret)
 		goto bail_pio;
 
@@ -1828,6 +2074,13 @@
 
 	/* drain user sdma queue */
 	if (fd->pq) {
+#ifdef QIB_CONFIG_KNX
+		/*
+		 * The thread should be stopped first before attempting
+		 * to clean the queue.
+		 */
+		qib_knx_sdma_queue_destroy(fd);
+#endif
 		qib_user_sdma_queue_drain(rcd->ppd, fd->pq);
 		qib_user_sdma_queue_destroy(fd->pq);
 	}
@@ -1885,6 +2138,12 @@
 	}
 
 	mutex_unlock(&qib_mutex);
+#ifdef QIB_CONFIG_KNX
+	if (fd->knx_node_id) {
+		qib_knx_free_ctxtdata(dd, rcd);
+		goto bail;
+	}
+#endif
 	qib_free_ctxtdata(dd, rcd); /* after releasing the mutex */
 
 bail:
@@ -2170,6 +2429,13 @@
 		ret = qib_assign_ctxt(fp, &cmd.cmd.user_info);
 		if (ret)
 			goto bail;
+#ifdef QIB_CONFIG_KNX
+		if (cmd.cmd.user_info.spu_knx_node_id)
+			ret = qib_get_early_base_info(
+				fp, (void __user *) (unsigned long)
+				cmd.cmd.user_info.spu_base_info,
+				cmd.cmd.user_info.spu_base_info_size);
+#endif
 		break;
 
 	case QIB_CMD_USER_INIT:
diff -ruN a9/drivers/infiniband/hw/qib/qib.h a10/drivers/infiniband/hw/qib/qib.h
--- a9/drivers/infiniband/hw/qib/qib.h	2015-09-10 09:35:36.420958201 -0700
+++ a10/drivers/infiniband/hw/qib/qib.h	2015-09-10 09:36:03.139932597 -0700
@@ -234,6 +234,10 @@
 	u32 lookaside_qpn;
 	/* QPs waiting for context processing */
 	struct list_head qp_wait_list;
+#ifdef QIB_CONFIG_KNX
+	/* KNX Receive Context Data */
+	struct qib_knx_ctxt *krcd;
+#endif
 #ifdef CONFIG_DEBUG_FS
 	/* verbs stats per CTX */
 	struct qib_opcode_stats_perctx *opstats;
@@ -1106,6 +1110,11 @@
 	struct kthread_worker *worker;
 
 	int assigned_node_id; /* NUMA node closest to HCA */
+
+#ifdef QIB_CONFIG_KNX
+	/* number of KNx nodes using this device */
+	u16 num_knx;
+#endif
 };
 
 /* hol_state values */
@@ -1134,6 +1143,9 @@
 	unsigned tidcursor;
 	struct qib_user_sdma_queue *pq;
 	int rec_cpu_num; /* for cpu affinity; -1 if none */
+#ifdef QIB_CONFIG_KNX
+	u16 knx_node_id;
+#endif
 };
 
 extern struct list_head qib_dev_list;
@@ -1211,6 +1223,13 @@
 	(((struct qib_filedata *)(fp)->private_data)->tidcursor)
 #define user_sdma_queue_fp(fp) \
 	(((struct qib_filedata *)(fp)->private_data)->pq)
+#ifdef QIB_CONFIG_KNX
+#define knx_node_fp(fp) \
+	(((struct qib_filedata *)(fp)->private_data)->knx_node_id)
+#else
+/* allow the use of knx_node_fp() outside of a #ifdef QIB_CONFIG_KNX */
+#define knx_node_fp(fp) 0
+#endif
 
 static inline struct qib_devdata *dd_from_ppd(struct qib_pportdata *ppd)
 {
diff -ruN a9/drivers/infiniband/hw/qib/qib_init.c a10/drivers/infiniband/hw/qib/qib_init.c
--- a9/drivers/infiniband/hw/qib/qib_init.c	2015-09-10 09:35:36.420958201 -0700
+++ a10/drivers/infiniband/hw/qib/qib_init.c	2015-09-10 09:36:03.140934324 -0700
@@ -51,6 +51,10 @@
 #include "qib_verbs.h"
 #endif
 
+#ifdef QIB_CONFIG_KNX
+#include "qib_knx.h"
+#endif
+
 #undef pr_fmt
 #define pr_fmt(fmt) QIB_DRV_NAME ": " fmt
 
@@ -1319,6 +1323,12 @@
 	/* not fatal if it doesn't work */
 	if (qib_init_qibfs())
 		pr_err("Unable to register ipathfs\n");
+
+#ifdef QIB_CONFIG_KNX
+	ret = qib_knx_server_init();
+	if (ret < 0)
+		pr_err(": Unable to start KNX listen thread\n");
+#endif
 	goto bail; /* all OK */
 
 bail_dev:
@@ -1346,6 +1356,9 @@
 {
 	int ret;
 
+#ifdef QIB_CONFIG_KNX
+	qib_knx_server_exit();
+#endif
 	ret = qib_exit_qibfs();
 	if (ret)
 		pr_err(
@@ -1589,6 +1602,9 @@
 	/* unregister from IB core */
 	qib_unregister_ib_device(dd);
 
+#ifdef QIB_CONFIG_KNX
+	qib_knx_remove_device(dd);
+#endif
 	/*
 	 * Disable the IB link, disable interrupts on the device,
 	 * clear dma engines, etc.
diff -ruN a9/drivers/infiniband/hw/qib/qib_knx.c a10/drivers/infiniband/hw/qib/qib_knx.c
--- a9/drivers/infiniband/hw/qib/qib_knx.c	1969-12-31 16:00:00.000000000 -0800
+++ a10/drivers/infiniband/hw/qib/qib_knx.c	2015-09-10 09:36:03.140934324 -0700
@@ -0,0 +1,1532 @@
+/*
+ * Copyright (c) 2012, 2013 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+#include <linux/module.h>
+#include <linux/kthread.h>
+#include <linux/kernel.h>
+#include <linux/dma-mapping.h>
+#include <linux/scatterlist.h>
+#include <modules/scif.h>
+
+#include "qib.h"
+#include "qib_knx.h"
+#include "qib_user_sdma.h"
+#include "qib_knx_common.h"
+
+unsigned int qib_knx_nconns = 5;
+module_param_named(num_conns, qib_knx_nconns, uint, S_IRUGO);
+MODULE_PARM_DESC(num_conns, "Max number of pending connections");
+
+#define QIB_KNX_SCIF_PORT SCIF_OFED_PORT_9
+#define CLIENT_THREAD_NAME(x) "qib/mic" __stringify(x)
+
+#define knx_sdma_next(sdma) \
+	(sdma->head = ((sdma->head + 1) % sdma->desc_num))
+#define per_ctxt(ctxt, sub) ((ctxt * QLOGIC_IB_MAX_SUBCTXT) + sub)
+#define QIB_KNX_SDMA_STATUS(sdma, st) \
+	QIB_KNX_SDMA_SET(sdma->mflags->status, ((u64)st << 32) | 1)
+
+struct qib_knx_server {
+	struct task_struct *kthread;
+	struct scif_pollepd epd;
+	spinlock_t client_lock;
+	struct list_head clients;
+	unsigned int nclients;
+};
+
+struct qib_knx_rma {
+	/* SCIF registered offset */
+	off_t offset;
+	/* size of mapped memory (in bytes) */
+	size_t size;
+	/* kernel virtual address of ioremap'ed memory */
+	void *kvaddr;
+};
+
+struct qib_knx_mem_map {
+	/* physical address is DMA range */
+	dma_addr_t dma_mapped_addr;
+	/* DMA direction */
+	enum dma_data_direction dir;
+	/* size of remote memory area */
+	size_t size;
+	/* SCIF array of physical pages */
+	struct scif_range *pages;
+};
+
+struct qib_knx_mem_map_sg {
+	/* list of pages to map */
+	struct scatterlist *sglist;
+	/* DMA direction */
+	enum dma_data_direction dir;
+	/* total size of mapped memory */
+	size_t size;
+	struct scif_range *pages;
+};
+
+struct qib_knx_tidrcv {
+	struct qib_knx_rma tidmem;
+	u64 tidbase;
+	u32 tidcnt;
+};
+
+struct qib_knx_ctxt {
+	u16 ctxt;
+	struct qib_knx *knx;
+	struct qib_pportdata *ppd;
+	/* local registered memory for PIO buffers */
+	struct qib_knx_rma piobufs[QLOGIC_IB_MAX_SUBCTXT];
+	/* local registered memory for user registers */
+	struct qib_knx_rma uregs;
+	/* local registered memory for PIO avail registers */
+	struct qib_knx_rma pioavail;
+	/* remote registered memory for RcvHdr Q */
+	struct qib_knx_mem_map_sg rcvhdrq;
+	/* remote registered memory for SendBuf status */
+	struct qib_knx_mem_map sbufstatus;
+	/* remote registered memory for RcvHdrTail register */
+	struct qib_knx_mem_map rcvhdrqtailaddr;
+	/* remote registered memory for Eager buffers */
+	struct qib_knx_mem_map_sg eagerbufs;
+
+	/* Saved offsets for shared context processes */
+	__u64 uregbase;
+	__u64 pioavailaddr;
+	__u64 status;
+	__u64 piobufbase[QLOGIC_IB_MAX_SUBCTXT];
+	__u32 runtime_flags;
+
+	struct qib_user_sdma_queue *pq[QLOGIC_IB_MAX_SUBCTXT];
+};
+
+struct qib_knx_sdma {
+	/* KNX flags page */
+	struct scif_range *mflag_pages;
+	struct qib_knx_sdma_mflags *mflags;
+	/* KNX descriptor queue */
+	struct scif_range *queue_pages;
+	struct qib_knx_sdma_desc *queue;
+	u32 desc_num;
+	/* host flags (in host memory) */
+	struct qib_knx_rma hflags_mem;
+	struct qib_knx_sdma_hflags *hflags;
+	u32 head;                           /* shadow */
+	u32 complete;
+};
+
+struct qib_knx {
+	struct list_head list;
+	struct scif_pollepd epd;
+	struct scif_portID peer;
+	struct scif_pci_info pci_info;
+	int numa_node;
+	struct qib_devdata *dd;
+	struct qib_knx_ctxt **ctxts;
+	spinlock_t ctxt_lock;
+	resource_size_t bar;
+	u64 barlen;
+	struct qib_knx_sdma *sdma;
+	struct task_struct *sdma_poll;
+	atomic_t tref;
+	char tname[64];
+	struct qib_knx_rma tidmem;
+};
+
+static struct qib_knx_server *server;
+
+static int qib_knx_init(struct qib_knx_server *);
+static void qib_knx_free(struct qib_knx *, int);
+static int qib_knx_server_listen(void *);
+static off_t qib_knx_register_memory(struct qib_knx *, struct qib_knx_rma *,
+				     void *, size_t, int, const char *);
+static int qib_knx_unregister_memory(struct qib_knx *, struct qib_knx_rma *,
+				     const char *);
+static __always_inline void qib_knx_memcpy(void *, void __iomem *, size_t);
+static ssize_t qib_show_knx_node(struct device *, struct device_attribute *,
+				 char *);
+static int qib_knx_sdma_init(struct qib_knx *);
+static void qib_knx_sdma_teardown(struct qib_knx *);
+static __always_inline struct page *
+qib_knx_phys_to_page(struct qib_knx *, unsigned long);
+static int qib_knx_sdma_pkts_to_descs(struct qib_knx_ctxt *,
+				      struct qib_knx_sdma_desc *,
+				      struct qib_user_sdma_queue *,
+				      int *, struct list_head *);
+static int qib_knx_sdma_poll(void *);
+static int qib_knx_tidrcv_init(struct qib_knx *);
+static int qib_knx_tidrcv_teardown(struct qib_knx *);
+
+inline struct qib_knx *qib_knx_get(u16 nodeid)
+{
+	struct qib_knx *knx = NULL;
+
+	spin_lock(&server->client_lock);
+	if (!list_empty(&server->clients))
+		list_for_each_entry(knx, &server->clients, list)
+			if (knx->peer.node == nodeid)
+				break;
+	spin_unlock(&server->client_lock);
+	return knx;
+}
+
+inline struct qib_devdata *qib_knx_node_to_dd(u16 node)
+{
+	struct qib_knx *knx = qib_knx_get(node);
+	return knx ? knx->dd : NULL;
+}
+
+static int qib_knx_init(struct qib_knx_server *server)
+{
+	int ret = 0, num_devs = 0, i, seen = 0;
+	unsigned fewest = -1U;
+	struct qib_devdata *dd = NULL, *dd_no_numa = NULL;
+	struct qib_knx *knx;
+	struct qib_device_info info = { -1 };
+
+	knx = kzalloc(sizeof(*knx), GFP_KERNEL);
+	if (!knx) {
+		ret = -ENOMEM;
+		goto bail;
+	}
+	ret = scif_accept(server->epd.epd, &knx->peer, &knx->epd.epd, 0);
+	if (ret) {
+		kfree(knx);
+		goto bail;
+	}
+
+	INIT_LIST_HEAD(&knx->list);
+	spin_lock_init(&knx->ctxt_lock);
+	knx->numa_node = -1;
+	ret = scif_pci_info(knx->peer.node, &knx->pci_info);
+	if (!ret) {
+		knx->numa_node = pcibus_to_node(knx->pci_info.pdev->bus);
+		knx->bar = pci_resource_start(knx->pci_info.pdev, 0);
+		knx->barlen = pci_resource_len(knx->pci_info.pdev, 0);
+	}
+
+	if (knx->numa_node < 0)
+		knx->numa_node = numa_node_id();
+
+	num_devs = qib_count_units(NULL, NULL);
+	if (unlikely(!num_devs)) {
+		ret = -ENODEV;
+		/* we have to send this */
+		scif_send(knx->epd.epd, &info, sizeof(info),
+			  SCIF_SEND_BLOCK);
+		goto done;
+	}
+
+	/*
+	 * Attempt to find an HCA on the same NUMA node as the card. Save
+	 * the first HCA that hasn't been associated with a card in case
+	 * there is no HCA on the same NUMA node.
+	 */
+	for (i = 0; seen < num_devs; i++) {
+		dd = qib_lookup(i);
+		if (dd) {
+			if (dd->assigned_node_id == knx->numa_node) {
+				knx->dd = dd;
+				break;
+			} else if (dd->num_knx < fewest)
+				dd_no_numa = dd;
+			seen++;
+		}
+	}
+	/*
+	 * We didn't find a QIB device on the same NUMA node,
+	 * use the "backup".
+	 */
+	if (unlikely(!knx->dd)) {
+		if (!dd_no_numa) {
+			ret = -ENODEV;
+			/* we have to send this */
+			scif_send(knx->epd.epd, &info, sizeof(info),
+				  SCIF_SEND_BLOCK);
+			goto done;
+		}
+		knx->dd = dd_no_numa;
+	}
+	knx->dd->num_knx++;
+
+	knx->ctxts = kzalloc_node(knx->dd->ctxtcnt * sizeof(*knx->ctxts),
+				  GFP_KERNEL, knx->numa_node);
+	if (!knx->ctxts)
+		ret = -ENOMEM;
+	/* Give the KNX the associated device information. */
+	info.unit = knx->dd->unit;
+	ret = scif_send(knx->epd.epd, &info, sizeof(info),
+			SCIF_SEND_BLOCK);
+
+	ret = qib_knx_sdma_init(knx);
+	if (ret)
+		goto done;
+	atomic_set(&knx->tref, 0);
+	ret = qib_knx_tidrcv_init(knx);
+done:
+	spin_lock(&server->client_lock);
+	list_add_tail(&knx->list, &server->clients);
+	server->nclients++;
+	spin_unlock(&server->client_lock);
+	try_module_get(THIS_MODULE);
+bail:
+	return ret;
+}
+
+static void qib_knx_free(struct qib_knx *knx, int unload)
+{
+	struct qib_devdata *dd = knx->dd;
+	int i;
+
+	qib_knx_tidrcv_teardown(knx);
+	qib_knx_sdma_teardown(knx);
+	if (dd)
+		dd->num_knx--;
+	/*
+	 * If this function is called with unload set, we can
+	 * free the context data. Otherwise, we are here
+	 * because the connection between the modules has broken.
+	 */
+	if (knx->ctxts && unload && dd)
+		for (i = dd->first_user_ctxt; i < dd->ctxtcnt; i++)
+			qib_knx_free_ctxtdata(dd, dd->rcd[i]);
+
+	scif_close(knx->epd.epd);
+	module_put(THIS_MODULE);
+	if (unload)
+		kfree(knx->ctxts);
+}
+
+static int qib_knx_server_listen(void *data)
+{
+	struct qib_knx_server *server =
+		(struct qib_knx_server *)data;
+	struct qib_knx *client, *ptr;
+	int ret = 0;
+
+	server->epd.epd = scif_open();
+	if (!server->epd.epd) {
+		ret = -EIO;
+		goto done;
+	}
+	server->epd.events = POLLIN;
+	ret = scif_bind(server->epd.epd, QIB_KNX_SCIF_PORT);
+	if (ret < 0)
+		goto err_close;
+
+	ret = scif_listen(server->epd.epd, qib_knx_nconns);
+	if (ret)
+		goto err_close;
+
+	while (!kthread_should_stop()) {
+		schedule();
+
+		/* poll for one millisecond. Is 50ms good? */
+		ret = scif_poll(&server->epd, 1, 50);
+		if (ret > 0)
+			ret = qib_knx_init(server);
+
+		/*
+		 * Check for any disconnected clients and clean them up.
+		 * Since there is nothing anywhere else that can change the
+		 * list, we only lock when we are deleting a client so
+		 * querying functions operate on "static" list.
+		 */
+		list_for_each_entry_safe(client, ptr, &server->clients, list) {
+			client->epd.events = POLLIN;
+			if (scif_poll(&client->epd, 1, 1)) {
+				if (client->epd.revents & POLLHUP) {
+					spin_lock(&server->client_lock);
+					list_del(&client->list);
+					spin_unlock(&server->client_lock);
+					qib_knx_free(client, 0);
+					kfree(client);
+				}
+			}
+		}
+	}
+err_close:
+	scif_close(server->epd.epd);
+done:
+	return ret;
+}
+
+
+static off_t qib_knx_register_memory(struct qib_knx *knx,
+				     struct qib_knx_rma *rma, void *kvaddr,
+				     size_t size, int prot, const char *what)
+{
+	int ret = 0;
+	off_t regoffset;
+
+	if (!kvaddr || ((unsigned long)kvaddr & ~PAGE_MASK)) {
+		ret = -EINVAL;
+		goto bail;
+	}
+	rma->kvaddr = kvaddr;
+	rma->size = size;
+
+	regoffset = scif_register(knx->epd.epd, rma->kvaddr, rma->size,
+				  0, prot, SCIF_MAP_KERNEL);
+	if (IS_ERR_VALUE(regoffset)) {
+		ret = regoffset;
+		goto bail;
+	}
+	rma->offset = regoffset;
+	return regoffset;
+bail:
+	rma->kvaddr = NULL;
+	rma->size = 0;
+	return ret;
+}
+
+static int qib_knx_unregister_memory(struct qib_knx *knx,
+				     struct qib_knx_rma *rma, const char *what)
+{
+	int ret = 0;
+
+	if (!rma) {
+		ret = -EINVAL;
+		goto done;
+	}
+	if (rma->offset)
+		ret = scif_unregister(knx->epd.epd, rma->offset, rma->size);
+	rma->kvaddr = NULL;
+	rma->size = 0;
+	rma->offset = 0;
+done:
+	return ret;
+}
+
+static __always_inline void qib_knx_memcpy(void *dst, void __iomem *src,
+					   size_t size)
+{
+	memcpy_fromio(dst, src, size);
+}
+
+int qib_knx_alloc_ctxt(u16 node_id, unsigned ctxt)
+{
+	struct qib_knx *knx = qib_knx_get(node_id);
+	struct qib_devdata *dd = knx->dd;
+	struct qib_knx_ctxt *ptr;
+	int ret = 0;
+
+	if (ctxt >= dd->ctxtcnt) {
+		ret = -EINVAL;
+		goto bail;
+	}
+	if (unlikely(!knx->ctxts)) {
+		ret = -ENOMEM;
+		goto bail;
+	}
+	ptr = kzalloc_node(sizeof(*ptr), GFP_KERNEL, knx->numa_node);
+	if (unlikely(!ptr)) {
+		ret = -ENOMEM;
+		goto bail;
+	}
+	ptr->knx = knx;
+	ptr->ctxt = ctxt;
+	ptr->ppd = dd->rcd[ctxt]->ppd;
+
+	spin_lock(&knx->ctxt_lock);
+	knx->ctxts[ctxt] = ptr;
+	dd->rcd[ctxt]->krcd = ptr;
+	spin_unlock(&knx->ctxt_lock);
+bail:
+	return ret;
+}
+
+__u64 qib_knx_ctxt_info(struct qib_ctxtdata *rcd,
+			enum qib_knx_ctxtinfo_type type,
+			struct file *fp)
+{
+	struct qib_knx *knx = rcd->krcd->knx;
+	__u16 subctxt;
+	__u64 ret = 0;
+
+	spin_lock(&knx->ctxt_lock);
+	if (!knx || !knx->ctxts || !knx->ctxts[rcd->ctxt])
+		goto done;
+
+	switch (type) {
+	case QIB_KNX_CTXTINFO_UREG:
+		ret = knx->ctxts[rcd->ctxt]->uregbase;
+		break;
+	case QIB_KNX_CTXTINFO_PIOAVAIL:
+		ret = knx->ctxts[rcd->ctxt]->pioavailaddr;
+		break;
+	case QIB_KNX_CTXTINFO_STATUS:
+		ret = knx->ctxts[rcd->ctxt]->status;
+		break;
+	case QIB_KNX_CTXTINFO_PIOBUFBASE:
+		subctxt = fp ? subctxt_fp(fp) : 0;
+		ret = knx->ctxts[rcd->ctxt]->piobufbase[subctxt];
+		break;
+	case QIB_KNX_CTXTINFO_FLAGS:
+		ret = knx->ctxts[rcd->ctxt]->runtime_flags;
+		break;
+	}
+done:
+	spin_unlock(&knx->ctxt_lock);
+	return ret;
+}
+
+int qib_knx_setup_piobufs(struct qib_devdata *dd, struct qib_ctxtdata *rcd,
+			  __u16 subctxt)
+{
+	unsigned piobufs, piocnt;
+	char buf[16];
+	off_t offset;
+	int ret = 0;
+	struct qib_knx *knx = rcd->krcd->knx;
+
+	if (unlikely(!knx)) {
+		ret = -ENODEV;
+		goto bail;
+	}
+	if (unlikely(!knx->ctxts[rcd->ctxt])) {
+		ret = -EINVAL;
+		goto bail;
+	}
+
+	/*
+	 * We don't calculate piobufs based on the rcd->piobufs like
+	 * everywhere else in the driver because rcd->piobufs is based
+	 * on the 2K PIO buffer virtual address. We just need an offset.
+	 */
+	piobufs = rcd->pio_base * dd->palign;
+	if (!rcd->subctxt_cnt)
+		piocnt = rcd->piocnt;
+	else if (!subctxt) {
+		piocnt = (rcd->piocnt / rcd->subctxt_cnt) +
+			(rcd->piocnt % rcd->subctxt_cnt);
+		piobufs += dd->palign * (rcd->piocnt - piocnt);
+	} else {
+		piocnt = rcd->piocnt / rcd->subctxt_cnt;
+		piobufs += dd->palign * piocnt * (subctxt - 1);
+	}
+
+	/* register PIO buffers */
+	snprintf(buf, sizeof(buf), "PIO bufs %u:%u", rcd->ctxt, subctxt);
+	offset = qib_knx_register_memory(
+		knx, &knx->ctxts[rcd->ctxt]->piobufs[subctxt],
+		dd->piobase + piobufs, piocnt * dd->palign,
+		SCIF_PROT_WRITE, buf);
+	if (IS_ERR_VALUE(offset)) {
+		ret = offset;
+		goto bail;
+	}
+	knx->ctxts[rcd->ctxt]->piobufbase[subctxt] = offset;
+bail:
+	return ret;
+}
+
+int qib_knx_setup_pioregs(struct qib_devdata *dd, struct qib_ctxtdata *rcd,
+			  struct qib_base_info *binfo)
+{
+	int ret = 0;
+	off_t offset;
+	struct qib_knx *knx = rcd->krcd->knx;
+
+	if (unlikely(!knx)) {
+		ret = -ENODEV;
+		goto bail;
+	}
+	if (unlikely(!knx->ctxts[rcd->ctxt])) {
+		ret = -EINVAL;
+		goto bail;
+	}
+
+	/* register the user registers to remote mapping */
+	offset = qib_knx_register_memory(knx, &knx->ctxts[rcd->ctxt]->uregs,
+					 (char *)dd->userbase +
+					 (dd->ureg_align * rcd->ctxt),
+					 dd->flags & QIB_HAS_HDRSUPP ?
+					 2 * PAGE_SIZE : PAGE_SIZE,
+					 SCIF_PROT_READ|SCIF_PROT_WRITE,
+					 "UserRegs");
+	if (IS_ERR_VALUE(offset)) {
+		ret = offset;
+		goto bail;
+	}
+	knx->ctxts[rcd->ctxt]->uregbase = offset;
+
+	/*
+	 * register the PIO availability registers.
+	 * user status 64bit values are part of the page containing the
+	 * pio availability registers.
+	 */
+	offset = qib_knx_register_memory(knx, &knx->ctxts[rcd->ctxt]->pioavail,
+					 (void *)dd->pioavailregs_dma,
+					 PAGE_SIZE, SCIF_PROT_READ,
+					 "pioavail regs");
+	if (IS_ERR_VALUE(offset)) {
+		ret = offset;
+		goto bail_uregs;
+	}
+	knx->ctxts[rcd->ctxt]->pioavailaddr = offset;
+	/*
+	 * User status bitmask is part of the same mapped page as the PIO
+	 * availability bits and user level code should know that. Therefore,
+	 * we just need to give it the offset into the mapped page where the
+	 * status mask is located.
+	 */
+	knx->ctxts[rcd->ctxt]->status = offset;
+	/* Record the run time flags that were passed in by the user. */
+	knx->ctxts[rcd->ctxt]->runtime_flags = binfo->spi_runtime_flags;
+	goto bail;
+bail_uregs:
+	qib_knx_unregister_memory(knx, &knx->ctxts[rcd->ctxt]->uregs,
+				  "UserRegs");
+bail:
+	return ret;
+}
+
+int qib_knx_create_rcvhdrq(struct qib_devdata *dd, struct qib_ctxtdata *rcd,
+			   struct qib_base_info *binfo)
+{
+	struct qib_knx_mem_map_sg *mapsg;
+	struct qib_knx_mem_map *map;
+	struct qib_knx *knx = rcd->krcd->knx;
+	dma_addr_t offset;
+	struct scatterlist *sg;
+	unsigned num_pages;
+	size_t size;
+	int ret = 0, i;
+
+	if (unlikely(!knx)) {
+		ret = -ENODEV;
+		goto bail;
+	}
+	if (unlikely(!knx->ctxts[rcd->ctxt])) {
+		ret = -EINVAL;
+		goto bail;
+	}
+	if (unlikely(!binfo->spi_rcvhdr_base)) {
+		ret = -EIO;
+		goto bail;
+	}
+
+	size = ALIGN(dd->rcvhdrcnt * dd->rcvhdrentsize *
+		     sizeof(u32), PAGE_SIZE);
+	mapsg = &knx->ctxts[rcd->ctxt]->rcvhdrq;
+	ret = scif_get_pages(knx->epd.epd, binfo->spi_rcvhdr_base,
+			     size, &mapsg->pages);
+	if (ret)
+		goto bail;
+	if (!mapsg->pages->nr_pages) {
+		rcd->rcvhdrq = NULL;
+		ret = -ENOMEM;
+		goto bail_rcvq_pages;
+	}
+	num_pages = mapsg->pages->nr_pages;
+	if (num_pages * PAGE_SIZE != size) {
+		ret = -EINVAL;
+		goto bail_rcvq_pages;
+	}
+	rcd->rcvhdrq_size = size;
+	/* verify that rcvhdr q is contiguous */
+	offset = mapsg->pages->phys_addr[0];
+	for (i = 1; i < num_pages; i++) {
+		if (offset + PAGE_SIZE != mapsg->pages->phys_addr[i]) {
+			ret = -EFAULT;
+			goto bail_rcvq_pages;
+		}
+		offset += PAGE_SIZE;
+	}
+	memset(mapsg->pages->va[0], 0, size);
+	mapsg->size = size;
+	mapsg->dir = DMA_FROM_DEVICE;
+	/*
+	 * Streaming DMa mappings are supposed to be short-lived.
+	 * The mappings here are not exactly short-lived and
+	 * technically we might not even need them since SusieQ
+	 * can use 64bit addresses for DMA but the CPU might not.
+	 * (see pci_set_dma_mask() in qib_pcie.c).
+	 */
+	mapsg->sglist = kzalloc_node(num_pages * sizeof(*mapsg->sglist),
+				     GFP_KERNEL, knx->numa_node);
+	if (!mapsg->sglist) {
+		ret = -ENOMEM;
+		goto bail_rcvq_pages;
+	}
+	sg_init_table(mapsg->sglist, num_pages);
+	for_each_sg(mapsg->sglist, sg, num_pages, i)
+		sg_set_page(sg, vmalloc_to_page(mapsg->pages->va[i]), PAGE_SIZE,
+			    0);
+	ret = pci_map_sg(dd->pcidev, mapsg->sglist, num_pages, mapsg->dir);
+	if (!ret) {
+		rcd->rcvhdrq_phys = 0;
+		goto bail_free_sgtable;
+	}
+	/*
+	 * pci_map_sg() will remap all 128 pages of the
+	 * scatterlist separately (without coalescing them).
+	 * However, since the buffer is contiguous, as long
+	 * as the base address is mapped correctly, everything
+	 * should work. In any case, check that the mapped
+	 * addresses are contiguous anyway.
+	 */
+	offset = sg_dma_address(mapsg->sglist);
+	for_each_sg(mapsg->sglist, sg, num_pages, i) {
+		dma_addr_t sgaddr;
+		sgaddr = sg_dma_address(sg);
+		if ((offset == sgaddr && i) ||
+		    (offset != sgaddr && sgaddr != offset + PAGE_SIZE)) {
+			ret = -EINVAL;
+			goto bail_rcvhdrq;
+		}
+		offset = sgaddr;
+	}
+	rcd->rcvhdrq_phys = sg_dma_address(mapsg->sglist);
+	rcd->rcvhdrq = mapsg->pages->va[0];
+
+	map = &knx->ctxts[rcd->ctxt]->sbufstatus;
+	ret = scif_get_pages(knx->epd.epd, binfo->spi_sendbuf_status,
+			     PAGE_SIZE, &map->pages);
+	if (ret)
+		goto bail_rcvhdrq;
+
+	map->size = PAGE_SIZE;
+	if (map->pages->nr_pages > 0) {
+		rcd->user_event_mask = map->pages->va[0];
+		/*
+		 * clear the mapped page - this is important as it will cause
+		 * user level to request "invalid" updates on every PIO send.
+		 */
+		memset(rcd->user_event_mask, 0, PAGE_SIZE);
+	}
+	/*
+	 * Map the rcvhdrtailaddr page(s) if we are goign to DMA the tail
+	 * register to memory, the chip will be prgrammed when
+	 * qib_do_user_init() calls f_rcvctrl().
+	 */
+	if (!(dd->flags & QIB_NODMA_RTAIL) && binfo->spi_rcvhdr_tailaddr) {
+		map = &knx->ctxts[rcd->ctxt]->rcvhdrqtailaddr;
+		ret = scif_get_pages(knx->epd.epd, binfo->spi_rcvhdr_tailaddr,
+				     PAGE_SIZE, &map->pages);
+		if (ret)
+			goto bail_umask;
+		map->size = PAGE_SIZE;
+		map->dir = DMA_FROM_DEVICE;
+		/* don't reuse num_pages in case there is an error */
+		if (map->pages->nr_pages > 0) {
+			rcd->rcvhdrqtailaddr_phys =
+				pci_map_page(dd->pcidev,
+					     vmalloc_to_page(map->pages->va[0]),
+					     0, map->size, map->dir);
+			if (pci_dma_mapping_error(dd->pcidev,
+						  rcd->rcvhdrqtailaddr_phys)) {
+				rcd->rcvhdrqtailaddr_phys = 0;
+				ret = -ENOMEM;
+				goto bail_tail;
+			}
+			rcd->rcvhdrtail_kvaddr = map->pages->va[0];
+			/* clear, just in case... */
+			memset(rcd->rcvhdrtail_kvaddr, 0, map->size);
+			map->dma_mapped_addr =
+				rcd->rcvhdrqtailaddr_phys;
+			knx->ctxts[rcd->ctxt]->runtime_flags &=
+				~QIB_RUNTIME_NODMA_RTAIL;
+		}
+	}
+	ret = 0;
+	goto bail;
+bail_tail:
+	scif_put_pages(knx->ctxts[rcd->ctxt]->rcvhdrqtailaddr.pages);
+bail_umask:
+	rcd->user_event_mask = NULL;
+	scif_put_pages(knx->ctxts[rcd->ctxt]->sbufstatus.pages);
+bail_rcvhdrq:
+	rcd->rcvhdrq = NULL;
+	pci_unmap_sg(dd->pcidev, knx->ctxts[rcd->ctxt]->rcvhdrq.sglist,
+		     num_pages, knx->ctxts[rcd->ctxt]->rcvhdrq.dir);
+bail_free_sgtable:
+	kfree(knx->ctxts[rcd->ctxt]->rcvhdrq.sglist);
+bail_rcvq_pages:
+	scif_put_pages(knx->ctxts[rcd->ctxt]->rcvhdrq.pages);
+bail:
+	return ret;
+}
+
+int qib_knx_setup_eagerbufs(struct qib_ctxtdata *rcd,
+			    struct qib_base_info *binfo)
+{
+	struct qib_knx_mem_map_sg *map;
+	struct scatterlist *sg;
+	struct qib_devdata *dd = rcd->dd;
+	struct qib_knx *knx = rcd->krcd->knx;
+	unsigned size, egrsize, egrcnt, num_pages, bufs_ppage,
+		egrbufcnt;
+	dma_addr_t dma_addr, page;
+	int ret = -ENOMEM, i, bufcnt;
+
+	if (unlikely(!knx)) {
+		ret = -ENODEV;
+		goto bail;
+	}
+	if (unlikely(!knx->ctxts[rcd->ctxt])) {
+		ret = -EINVAL;
+		goto bail;
+	}
+	if (unlikely(!binfo->spi_rcv_egrbufs)) {
+		ret = -ENOBUFS;
+		goto bail;
+	}
+	size = binfo->spi_rcv_egrbuftotlen;
+	egrsize = dd->rcvegrbufsize;
+	egrcnt = rcd->rcvegrcnt;
+
+	/*
+	 * Check whether the total size of the buffer is enough for all
+	 * Eager buffers.
+	 */
+	if (size < egrsize * egrcnt) {
+		ret = -EINVAL;
+		goto bail;
+	}
+
+	/* number of pages required to fit all the eager buffers */
+	num_pages = (egrsize * egrcnt) / PAGE_SIZE;
+	/* number of buffers per page (depends on MTU) */
+	bufs_ppage = PAGE_SIZE / egrsize;
+	map = &knx->ctxts[rcd->ctxt]->eagerbufs;
+	ret = scif_get_pages(knx->epd.epd, binfo->spi_rcv_egrbufs,
+			     size, &map->pages);
+	if (ret)
+		goto bail;
+
+	if (map->pages->nr_pages != num_pages) {
+		ret = -EINVAL;
+		goto bail_free_scif;
+	}
+
+	/*
+	 * Allocate pointer to the pages from the KNX memory.
+	 * In the case of KNX eager buffers, we are not dealing with
+	 * 32K chunks of locally allocated memory. Therefore, we
+	 * allocate num_pages pointers instead of rcd->rcvegrbuf_chunks.
+	 */
+	if (likely(!rcd->rcvegrbuf)) {
+		rcd->rcvegrbuf = kzalloc_node(num_pages *
+					      sizeof(rcd->rcvegrbuf[0]),
+					      GFP_KERNEL, rcd->node_id);
+		if (!rcd->rcvegrbuf) {
+			ret = -ENOMEM;
+			goto bail_free_scif;
+		}
+	}
+
+	/*
+	 * Allocate array of DMA addresses for each of the mapped
+	 * pages.
+	 */
+	if (likely(!rcd->rcvegrbuf_phys)) {
+		rcd->rcvegrbuf_phys =
+			kzalloc_node(num_pages * sizeof(rcd->rcvegrbuf_phys[0]),
+				     GFP_KERNEL, rcd->node_id);
+		if (!rcd->rcvegrbuf_phys) {
+			ret = -ENOMEM;
+			goto bail_free_rcvegr;
+		}
+	}
+
+	map->size = size;
+	map->dir = DMA_BIDIRECTIONAL;
+	map->sglist = kzalloc_node(num_pages * sizeof(*map->sglist), GFP_KERNEL,
+				   knx->numa_node);
+	if (!map->sglist) {
+		ret = -ENOMEM;
+		goto bail_free_rcvegr_phys;
+	}
+	sg_init_table(map->sglist, num_pages);
+	for_each_sg(map->sglist, sg, num_pages, i) {
+		memset(map->pages->va[i], 0, PAGE_SIZE);
+		sg_set_page(sg, vmalloc_to_page(map->pages->va[i]),
+			    PAGE_SIZE, 0);
+	}
+	ret = pci_map_sg(dd->pcidev, map->sglist, num_pages, map->dir);
+	if (!ret) {
+		ret = -ENOMEM;
+		goto bail_free_rcvegr_phys;
+	}
+	for_each_sg(map->sglist, sg, num_pages, i) {
+		rcd->rcvegrbuf_phys[i] = sg_dma_address(sg);
+		rcd->rcvegrbuf[i] = map->pages->va[i];
+	}
+
+	for (egrbufcnt = i = 0; i < num_pages; i++) {
+		page = rcd->rcvegrbuf_phys[i];
+		dma_addr = page;
+		for (bufcnt = 0; egrbufcnt < egrcnt && bufcnt < bufs_ppage;
+		     egrbufcnt++, bufcnt++) {
+			dd->f_put_tid(dd, rcd->rcvegr_tid_base +
+					   egrbufcnt +
+					   (u64 __iomem *)((char __iomem *)
+							   dd->kregbase +
+							   dd->rcvegrbase),
+					   RCVHQ_RCV_TYPE_EAGER, dma_addr);
+			dma_addr += egrsize;
+		}
+	}
+	ret = 0;
+	goto bail;
+bail_free_rcvegr_phys:
+	kfree(map->sglist);
+	kfree(rcd->rcvegrbuf_phys);
+	rcd->rcvegrbuf_phys = NULL;
+bail_free_rcvegr:
+	kfree(rcd->rcvegrbuf);
+	rcd->rcvegrbuf = NULL;
+bail_free_scif:
+	scif_put_pages(map->pages);
+bail:
+	return ret;
+}
+
+void qib_knx_free_ctxtdata(struct qib_devdata *dd, struct qib_ctxtdata *rcd)
+{
+	struct qib_knx *knx = rcd->krcd->knx;
+	struct qib_knx_ctxt *ctxt;
+	char buf[16];
+	int i, ret = 0;
+
+	if (!rcd || !knx || !knx->ctxts)
+		return;
+
+	spin_lock(&knx->ctxt_lock);
+	ctxt = knx->ctxts[rcd->ctxt];
+	knx->ctxts[rcd->ctxt] = NULL;
+	spin_unlock(&knx->ctxt_lock);
+
+	if (!ctxt)
+		return;
+
+	if (rcd->rcvhdrq) {
+		/* Unmap the RcvHdr Q */
+		pci_unmap_sg(dd->pcidev, ctxt->rcvhdrq.sglist,
+			     ctxt->rcvhdrq.pages->nr_pages,
+			     ctxt->rcvhdrq.dir);
+		/* TODO: do something with return value */
+		ret = scif_put_pages(ctxt->rcvhdrq.pages);
+		kfree(ctxt->rcvhdrq.sglist);
+	}
+
+	if (rcd->user_event_mask)
+		/* TODO: do something with return value */
+		ret = scif_put_pages(ctxt->sbufstatus.pages);
+
+	if (rcd->rcvhdrtail_kvaddr) {
+		pci_unmap_page(dd->pcidev,
+			       ctxt->rcvhdrqtailaddr.dma_mapped_addr,
+			       ctxt->rcvhdrqtailaddr.size,
+			       ctxt->rcvhdrqtailaddr.dir);
+		/* TODO: do something with return value */
+		ret = scif_put_pages(ctxt->rcvhdrqtailaddr.pages);
+	}
+
+	if (rcd->rcvegrbuf) {
+		pci_unmap_sg(dd->pcidev, ctxt->eagerbufs.sglist,
+			     ctxt->eagerbufs.pages->nr_pages,
+			     ctxt->eagerbufs.dir);
+		/* TODO: do something with return value */
+		ret = scif_put_pages(ctxt->eagerbufs.pages);
+		kfree(ctxt->eagerbufs.sglist);
+		kfree(rcd->rcvegrbuf);
+		kfree(rcd->rcvegrbuf_phys);
+	}
+
+	/* We are done with all remote memory, handle local */
+	qib_knx_unregister_memory(knx, &ctxt->pioavail, "pioavail regs");
+	qib_knx_unregister_memory(knx, &ctxt->uregs, "UserRegs");
+	for (i = 0; i < QLOGIC_IB_MAX_SUBCTXT; i++) {
+		snprintf(buf, sizeof(buf), "PIO bufs %u:%u", rcd->ctxt, i);
+		qib_knx_unregister_memory(knx, &ctxt->piobufs[i], buf);
+	}
+
+	kfree(ctxt);
+	kfree(rcd);
+}
+
+/*
+ * TID management for processes on the MIC happens on the MIC. Therefore,
+ * we only register the HW TID array here.
+ * The MIC will calculate TID array offsets using the same algorithm is
+ * the host. Therefore, it is OK that the entire HW TID array is mapped
+ * since neither side should step on the other.
+ */
+static int qib_knx_tidrcv_init(struct qib_knx *knx)
+{
+	struct qib_devdata *dd = knx->dd;
+	struct qib_knx_tid_info info;
+	void *tidbase;
+	int ret = 0;
+	off_t offset = 0;
+	size_t len;
+	char buf[64];
+
+	memset(&info, 0, sizeof(info));
+
+	info.tidcnt = dd->rcvtidcnt;
+	tidbase = ((char *)dd->kregbase + dd->rcvtidbase);
+	info.tidbase_len = dd->ctxtcnt * dd->rcvtidcnt * sizeof(tidbase);
+	info.tidtemplate = dd->tidtemplate;
+	info.invalidtid = dd->tidinvalid;
+	/* information needed to properly calculate DMA address to MIC pages */
+	info.bar_addr = knx->bar;
+	info.bar_len = knx->barlen;
+
+	snprintf(buf, sizeof(buf), "TID array KNx%u", knx->peer.node);
+	offset = qib_knx_register_memory(knx, &knx->tidmem, tidbase,
+					 info.tidbase_len, SCIF_PROT_WRITE,
+					 buf);
+	info.tidbase_offset = offset;
+	if (IS_ERR_VALUE(offset))
+		ret = offset;
+	len = scif_send(knx->epd.epd, &info, sizeof(info),
+			SCIF_SEND_BLOCK);
+	if (len < sizeof(info))
+		ret = -EFAULT;
+	return ret;
+}
+
+static int qib_knx_tidrcv_teardown(struct qib_knx *knx)
+{
+	char buf[64];
+	snprintf(buf, sizeof(buf), "TID array KNx%u", knx->peer.node);
+	return qib_knx_unregister_memory(knx, &knx->tidmem, buf);
+}
+
+static int qib_knx_sdma_init(struct qib_knx *knx)
+{
+	struct qib_knx_host_mem flags;
+	struct qib_knx_knc_mem mflags;
+	struct qib_knx_sdma *sdma;
+	char buf[64];
+	int ret = 0;
+
+	sdma = kzalloc_node(sizeof(*sdma), GFP_KERNEL, knx->numa_node);
+	if (!sdma) {
+		ret = -ENOMEM;
+		goto done;
+	}
+	sdma->hflags = kzalloc_node(PAGE_SIZE, GFP_KERNEL, knx->numa_node);
+	if (!sdma->hflags) {
+		ret = -ENOMEM;
+		goto done_free;
+	}
+	snprintf(buf, sizeof(buf), "Host SDMA flags KNx%u", knx->peer.node);
+	flags.flags_offset = qib_knx_register_memory(knx, &sdma->hflags_mem,
+						     sdma->hflags,
+						     PAGE_SIZE,
+						     SCIF_PROT_WRITE,
+						     buf);
+	if (IS_ERR_VALUE(flags.flags_offset)) {
+		ret = flags.flags_offset;
+		goto free_flags;
+	}
+	sdma->desc_num = knx->dd->pport[0].sdma_descq_cnt;
+	flags.desc_num = sdma->desc_num;
+	ret = scif_send(knx->epd.epd, &flags, sizeof(flags),
+			SCIF_SEND_BLOCK);
+	if (ret < sizeof(flags))
+		goto unregister;
+	ret = scif_recv(knx->epd.epd, &mflags, sizeof(mflags),
+			SCIF_RECV_BLOCK);
+	if (ret < sizeof(mflags)) {
+		ret = -EINVAL;
+		goto unregister;
+	}
+	ret = scif_get_pages(knx->epd.epd, mflags.flags_offset,
+			     PAGE_SIZE, &sdma->mflag_pages);
+	if (ret < 0 || !sdma->mflag_pages->nr_pages) {
+		ret = -EFAULT;
+		goto unregister;
+	}
+	sdma->mflags = sdma->mflag_pages->va[0];
+	ret = scif_get_pages(knx->epd.epd, mflags.queue_offset,
+			     mflags.queue_len, &sdma->queue_pages);
+	if (ret < 0)
+		goto put_flags;
+	if ((sdma->queue_pages->nr_pages * PAGE_SIZE) !=
+	    mflags.queue_len) {
+		ret = -EFAULT;
+		goto put_queue;
+	}
+	sdma->queue = sdma->queue_pages->va[0];
+	sdma->complete = -1;
+	sdma->head = -1;
+	/* set the initial trigger value */
+	QIB_KNX_SDMA_SET(sdma->hflags->trigger, -1);
+	QIB_KNX_SDMA_SET(sdma->mflags->complete, sdma->complete);
+	snprintf(knx->tname, sizeof(knx->tname), "qib/mic%u/poll",
+		 knx->peer.node);
+	knx->sdma = sdma;
+	ret = 0;
+	goto done;
+put_queue:
+	scif_put_pages(sdma->queue_pages);
+put_flags:
+	scif_put_pages(sdma->mflag_pages);
+unregister:
+	qib_knx_unregister_memory(knx, &sdma->hflags_mem, buf);
+free_flags:
+	kfree(sdma->hflags);
+done_free:
+	kfree(sdma);
+done:
+	/*
+	 * we have to respond to the MIC so it doesn't get stuck
+	 * in the scif_recv call
+	 */
+	scif_send(knx->epd.epd, &ret, sizeof(ret), SCIF_SEND_BLOCK);
+	return ret;
+}
+
+static void qib_knx_sdma_teardown(struct qib_knx *knx)
+{
+	int ret;
+	if (knx->sdma_poll)
+		ret = kthread_stop(knx->sdma_poll);
+	if (knx->sdma) {
+		if (knx->sdma->queue_pages->nr_pages) {
+			knx->sdma->queue = NULL;
+			scif_put_pages(knx->sdma->queue_pages);
+		}
+		if (knx->sdma->mflag_pages->nr_pages) {
+			knx->sdma->mflags = NULL;
+			scif_put_pages(knx->sdma->mflag_pages);
+		}
+		kfree(knx->sdma->hflags);
+		kfree(knx->sdma);
+		knx->sdma = NULL;
+	}
+}
+
+int qib_knx_sdma_queue_create(struct file *fd)
+{
+	struct qib_ctxtdata *rcd = ctxt_fp(fd);
+	struct qib_devdata *dd = rcd->dd;
+	struct qib_knx *knx = rcd->krcd->knx;
+	struct qib_knx_ctxt *ctxt = knx->ctxts[rcd->ctxt];
+	u8 subctxt = subctxt_fp(fd);
+	int ret = 0;
+
+	if (!ctxt) {
+		ret = -EINVAL;
+		goto done;
+	}
+	ctxt->pq[subctxt] = qib_user_sdma_queue_create(&dd->pcidev->dev,
+						       dd->unit, rcd->ctxt,
+						       subctxt);
+	if (!ctxt->pq[subctxt])
+		ret = -ENOMEM;
+	user_sdma_queue_fp(fd) = ctxt->pq[subctxt];
+	/*
+	 * We start the polling thread the first time a user SDMA
+	 * queue is created. There is no reason to take up CPU
+	 * cycles before then.
+	 */
+	if (atomic_inc_return(&knx->tref) == 1) {
+		knx->sdma_poll = kthread_run(qib_knx_sdma_poll, knx,
+					     knx->tname);
+		if (IS_ERR(knx->sdma_poll)) {
+			ret = -PTR_ERR(knx->sdma_poll);
+			atomic_dec(&knx->tref);
+			goto free_queue;
+		}
+	}
+	goto done;
+free_queue:
+	user_sdma_queue_fp(fd) = NULL;
+	qib_user_sdma_queue_destroy(ctxt->pq[subctxt]);
+	ctxt->pq[subctxt] = NULL;
+done:
+	return ret;
+}
+
+void qib_knx_sdma_queue_destroy(struct qib_filedata *fd)
+{
+	struct qib_ctxtdata *rcd = fd->rcd;
+	struct qib_knx *knx;
+	unsigned ctxt = rcd->ctxt, subctxt = fd->subctxt;
+
+	/* Host processes do not have a KNX rcd pointer. */
+	if (!rcd->krcd)
+		return;
+	knx = rcd->krcd->knx;
+	/* We still have the memory pointer through fd->pq */
+	spin_lock(&knx->ctxt_lock);
+	if (knx->ctxts[ctxt])
+		knx->ctxts[ctxt]->pq[subctxt] = NULL;
+	spin_unlock(&knx->ctxt_lock);
+	if (atomic_dec_and_test(&knx->tref)) {
+		int ret = kthread_stop(knx->sdma_poll);
+		knx->sdma_poll = NULL;
+	}
+}
+
+/*
+ * Convert a MIC physical address to the corresponding host page.
+ */
+static __always_inline struct page *
+qib_knx_phys_to_page(struct qib_knx *knx, unsigned long addr) {
+	unsigned long paddr;
+	if ((knx->bar + addr + PAGE_SIZE) >
+	    (knx->bar + knx->barlen))
+		return NULL;
+	paddr = knx->bar + addr;
+	return pfn_to_page(paddr >> PAGE_SHIFT);
+}
+
+static int qib_knx_sdma_pkts_to_descs(struct qib_knx_ctxt *ctxt,
+				      struct qib_knx_sdma_desc *desc,
+				      struct qib_user_sdma_queue *pq,
+				      int *ndesc, struct list_head *list)
+{
+	struct qib_knx *knx = ctxt->knx;
+	struct qib_user_sdma_pkt *pkt;
+	dma_addr_t pbc_dma_addr;
+	unsigned pktnw, pbcnw;
+	u32 counter;
+	u16 frag_size;
+	int ret = 0;
+	__le32 *pbc;
+
+	counter = pq->counter;
+
+	pbc = qib_user_sdma_alloc_header(pq, desc->pbclen, &pbc_dma_addr);
+	if (!pbc) {
+		ret = -ENOMEM;
+		goto done;
+	}
+	memcpy(pbc, desc->pbc, desc->pbclen);
+
+	pktnw = (le32_to_cpu(*pbc) & 0xFFFF);
+	/*
+	 * This assignment is a bit strange.  it's because the
+	 * the pbc counts the number of 32 bit words in the full
+	 * packet _except_ the first word of the pbc itself...
+	 */
+	pbcnw = (desc->pbclen >> 2) - 1;
+
+	if (pktnw < pbcnw) {
+		ret = -EINVAL;
+		goto free_pbc;
+	}
+
+	if (pktnw != ((desc->length >> 2) + pbcnw)) {
+		ret = -EINVAL;
+		goto free_pbc;
+	}
+
+	frag_size = (le32_to_cpu(*pbc)>>16) & 0xFFFF;
+	if (((frag_size ? frag_size : desc->length) + desc->pbclen) >
+	    ctxt->ppd->ibmaxlen) {
+		ret = -EINVAL;
+		goto free_pbc;
+	}
+	if (frag_size) {
+		/* new SDMA "protocol" */
+		unsigned pktsize, n;
+
+		n = desc->npages * ((2 * PAGE_SIZE / frag_size) + 1);
+		pktsize = sizeof(*pkt) + sizeof(pkt->addr[0]) * n;
+
+		pkt = kzalloc(pktsize + desc->tidlen, GFP_KERNEL);
+		if (!pkt) {
+			ret = -ENOMEM;
+			goto free_pbc;
+		}
+		pkt->largepkt = 1;
+		pkt->frag_size = frag_size;
+		pkt->addrlimit = n + ARRAY_SIZE(pkt->addr);
+
+		if (desc->tidlen) {
+			char *tidsmptr = (char *)pkt + pktsize;
+			memcpy(tidsmptr, desc->tidsm, desc->tidlen);
+			pkt->tidsm =
+				(struct qib_tid_session_member *)tidsmptr;
+			pkt->tidsmcount = desc->tidlen /
+				sizeof(*desc->tidsm);
+			pkt->tidsmidx = 0;
+		}
+		*pbc = cpu_to_le32(le32_to_cpu(*pbc) & 0x0000FFFF);
+	} else {
+		/* old SDMA */
+		pkt = kmem_cache_alloc(pq->pkt_slab, GFP_KERNEL);
+		if (!pkt) {
+			ret = -ENOMEM;
+			goto free_pbc;
+		}
+		pkt->largepkt = 0;
+		pkt->frag_size = desc->length;
+		pkt->addrlimit = ARRAY_SIZE(pkt->addr);
+	}
+	pkt->bytes_togo = desc->length;
+	pkt->payload_size = 0;
+	pkt->counter = counter;
+	pkt->tiddma = !!desc->tidlen;
+	/*
+	 * The generic user SDMA code will use this as a flag to
+	 * decide whether to call the KNx-specific pkt free
+	 * function. However, it doesn't know what the value
+	 * actually means.
+	 */
+	pkt->remote = (u64)knx;
+
+	qib_user_sdma_init_frag(pkt, 0,
+				0, desc->pbclen,
+				1, 0,
+				0, 0,
+				NULL, pbc,
+				pbc_dma_addr, desc->pbclen);
+	pkt->index = 0;
+	pkt->naddr = 1;
+
+	if (desc->npages) {
+		/* we have user data */
+		int i;
+		struct page *page;
+		unsigned plen = 0, len = desc->length;
+		for (i = 0; i < desc->npages; i++) {
+			unsigned long off = (i == 0 ? desc->offset : 0);
+			plen = (len > PAGE_SIZE ? PAGE_SIZE : len);
+			page = qib_knx_phys_to_page(knx, desc->pages[i]);
+			ret = qib_user_sdma_page_to_frags(knx->dd, pq,
+				   pkt, page, 0, off,
+				   (off + plen > PAGE_SIZE ?
+				    PAGE_SIZE - off : plen),
+				   NULL);
+			if (ret < 0)
+				goto free_sdma;
+			len -= plen - off;
+		}
+	} else {
+		pkt->addr[0].last_desc = 1;
+		if (pbc_dma_addr == 0) {
+			pbc_dma_addr = dma_map_single(&knx->dd->pcidev->dev,
+						      pbc, desc->pbclen,
+						      DMA_TO_DEVICE);
+			if (dma_mapping_error(&knx->dd->pcidev->dev,
+					      pbc_dma_addr)) {
+				ret = -ENOMEM;
+				goto free_sdma;
+			}
+			pkt->addr[0].addr = pbc_dma_addr;
+			pkt->addr[0].dma_mapped = 1;
+		}
+	}
+	counter++;
+	pkt->pq = pq;
+	pkt->index = 0;
+	*ndesc = pkt->naddr;
+
+	list_add_tail(&pkt->list, list);
+	goto done;
+free_sdma:
+	if (pkt->largepkt)
+		kfree(pkt);
+	else
+		kmem_cache_free(pq->pkt_slab, pkt);
+free_pbc:
+	if (pbc_dma_addr)
+		dma_pool_free(pq->header_cache, pbc, pbc_dma_addr);
+	else
+		kfree(pbc);
+done:
+	return ret;
+}
+
+void qib_knx_sdma_free_pkt(struct qib_user_sdma_pkt *pkt)
+{
+	struct qib_knx *knx = (struct qib_knx *)pkt->remote;
+	struct qib_knx_sdma *sdma = knx->sdma;
+	sdma_next(sdma, complete);
+	QIB_KNX_SDMA_SET(sdma->mflags->complete, sdma->complete);
+}
+
+static int qib_knx_sdma_poll(void *data)
+{
+	struct qib_knx *knx = (struct qib_knx *)data;
+	struct qib_knx_ctxt *ctxt;
+	struct qib_knx_sdma_desc desc;
+	struct qib_knx_sdma *sdma = knx->sdma;
+	struct qib_user_sdma_queue *pq;
+	struct list_head list;
+	u32 new_head;
+	int ret = 0, ndesc = 0, added;
+
+	if (!sdma)
+		return -EFAULT;
+
+	while (!kthread_should_stop()) {
+		added = 0;
+		new_head = QIB_KNX_SDMA_VALUE(sdma->hflags->trigger);
+		while (sdma->head != new_head) {
+			knx_sdma_next(sdma);
+			qib_knx_memcpy(&desc, sdma->queue + sdma->head,
+				       sizeof(desc));
+			if (!desc.ctxt) {
+				QIB_KNX_SDMA_STATUS(sdma, -EINVAL);
+				continue;
+			}
+			spin_lock(&knx->ctxt_lock);
+			ctxt = knx->ctxts[desc.ctxt];
+			if (!ctxt) {
+				/* we should never get here */
+				QIB_KNX_SDMA_STATUS(sdma, -EINVAL);
+				goto done_unlock;
+			}
+			pq = ctxt->pq[desc.subctxt];
+			if (!pq) {
+				QIB_KNX_SDMA_STATUS(sdma, -EFAULT);
+				goto done_unlock;
+			}
+			mutex_lock(&pq->lock);
+			if (pq->added > ctxt->ppd->sdma_descq_removed)
+				qib_user_sdma_hwqueue_clean(ctxt->ppd);
+			if (pq->num_sending)
+				qib_user_sdma_queue_clean(ctxt->ppd, pq);
+
+			INIT_LIST_HEAD(&list);
+			ret = qib_knx_sdma_pkts_to_descs(ctxt, &desc, pq,
+							 &ndesc, &list);
+			QIB_KNX_SDMA_STATUS(sdma, ret);
+			if (!list_empty(&list)) {
+				if (qib_sdma_descq_freecnt(ctxt->ppd) <
+				    ndesc) {
+					qib_user_sdma_hwqueue_clean(
+						ctxt->ppd);
+					if (pq->num_sending)
+						qib_user_sdma_queue_clean(
+							ctxt->ppd, pq);
+				}
+				ret = qib_user_sdma_push_pkts(ctxt->ppd,
+							      pq, &list, 1);
+				if (ret < 0)
+					goto free_pkts;
+				else {
+					pq->counter++;
+					added++;
+				}
+			}
+free_pkts:
+			if (!list_empty(&list))
+				qib_user_sdma_free_pkt_list(
+					&knx->dd->pcidev->dev, pq, &list);
+			mutex_unlock(&pq->lock);
+done_unlock:
+			spin_unlock(&knx->ctxt_lock);
+		}
+		if (!added) {
+			int i;
+			/*
+			 * Push the queues along
+			 * The polling thread will enter the inner loop only
+			 * if the KNX has posted new descriptors to the queue.
+			 * However, any packets that have been completed by
+			 * the HW need to be cleaned and that won't happen
+			 * unless we explicitly check.
+			 */
+			for (i = 0;
+			     i < knx->dd->ctxtcnt * QLOGIC_IB_MAX_SUBCTXT;
+			     i++) {
+				int c = i / QLOGIC_IB_MAX_SUBCTXT,
+					s = i % QLOGIC_IB_MAX_SUBCTXT;
+				spin_lock(&knx->ctxt_lock);
+				ctxt = knx->ctxts[c];
+				if (!ctxt)
+					goto loop_unlock;
+				pq = ctxt->pq[s];
+				if (!pq)
+					goto loop_unlock;
+				mutex_lock(&pq->lock);
+				if (pq->num_sending)
+					qib_user_sdma_queue_clean(ctxt->ppd,
+								  pq);
+				mutex_unlock(&pq->lock);
+loop_unlock:
+				spin_unlock(&knx->ctxt_lock);
+			}
+			might_sleep();
+		}
+	}
+	return ret;
+}
+
+void qib_knx_remove_device(struct qib_devdata *dd)
+{
+	if (server && dd->num_knx) {
+		struct qib_knx *knx, *knxp;
+		list_for_each_entry_safe(knx, knxp, &server->clients, list) {
+			if (knx->dd == dd) {
+				spin_lock(&server->client_lock);
+				list_del(&knx->list);
+				server->nclients--;
+				spin_unlock(&server->client_lock);
+				qib_knx_free(knx, 0);
+				kfree(knx);
+			}
+		}
+	}
+	return;
+}
+
+int __init qib_knx_server_init(void)
+{
+	server = kzalloc(sizeof(struct qib_knx_server), GFP_KERNEL);
+	if (!server)
+		return -ENOMEM;
+	INIT_LIST_HEAD(&server->clients);
+	spin_lock_init(&server->client_lock);
+	server->kthread = kthread_run(qib_knx_server_listen,
+				      server, CLIENT_THREAD_NAME(0));
+	if (IS_ERR(server->kthread))
+		return -PTR_ERR(server->kthread);
+	return 0;
+}
+
+void __exit qib_knx_server_exit(void)
+{
+	if (server) {
+		struct qib_knx *t, *tt;
+		/* Stop the thread so we don't accept any new connections. */
+		kthread_stop(server->kthread);
+		list_for_each_entry_safe(t, tt, &server->clients, list) {
+			spin_lock(&server->client_lock);
+			list_del(&t->list);
+			spin_unlock(&server->client_lock);
+			qib_knx_free(t, 1);
+			kfree(t);
+		}
+		kfree(server);
+	}
+}
diff -ruN a9/drivers/infiniband/hw/qib/qib_knx_common.h a10/drivers/infiniband/hw/qib/qib_knx_common.h
--- a9/drivers/infiniband/hw/qib/qib_knx_common.h	1969-12-31 16:00:00.000000000 -0800
+++ a10/drivers/infiniband/hw/qib/qib_knx_common.h	2015-09-10 09:36:03.140934324 -0700
@@ -0,0 +1,126 @@
+/*
+ * Copyright (c) 2013 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+#ifndef _QIB_KNX_COMMON_H
+#define _QIB_KNX_COMMON_H
+
+struct qib_device_info {
+	u16 unit;
+};
+
+#define QIB_SDMA_MAX_NPAGES 33
+#define QIB_KNX_SDMA_VALUE(fld) ((volatile u64)fld)
+#define QIB_KNX_SDMA_SET(fld, val)		\
+	do {					\
+		fld = (u64)(val);		\
+		smp_mb();			\
+	} while (0)
+
+struct qib_knx_host_mem {
+	off_t flags_offset;
+	unsigned desc_num;
+};
+
+struct qib_knx_knc_mem {
+	off_t flags_offset;
+	off_t queue_offset;
+	size_t queue_len;
+};
+
+struct qib_tid_sm {
+	__u16 tid;
+	__u16 offset;
+	__u16 length;
+};
+
+/*
+ * SDMA transfer descriptor. This structure communicates the SDMA
+ * transfers from the MIC to the host. It is very important for
+ * performance reasons that its size is multiple of 64B in order
+ * to guarantee proper alignment in the descriptor array.
+ */
+struct qib_knx_sdma_desc {
+	u16 ctxt;
+	u16 subctxt;
+	u32 pbclen;
+	__le32 pbc[16];
+	u64 length;
+	u32 npages;
+	unsigned tidlen;
+	off_t offset;
+	unsigned long pages[QIB_SDMA_MAX_NPAGES];
+	/* This array is 198B so the compiler will pad
+	 * it by 2B to make it multiple of 8B. */
+	struct qib_tid_sm tidsm[QIB_SDMA_MAX_NPAGES];
+	/*
+	 * The two paddings below are included in order to
+	 * make the size of the entire struct 576B (multiple
+	 * of 64B). The goal is that all elements in an array
+	 * of struct qib_knx_sdma_desc are 64B aligned.
+	 */
+	u16 __padding0;
+	u64 __padding1[2];
+};
+
+/*
+ * trigger, status, and complete fields are by 8 to be
+ * cacheline size.
+ */
+struct qib_knx_sdma_hflags {
+	u64 trigger;
+	u64 __padding[7];
+};
+
+#define sdma_next(s, fld) \
+	((s)->fld = (((s)->fld + 1) == (s)->desc_num) ? 0 : ((s)->fld + 1))
+
+struct qib_knx_sdma_mflags {
+	u64 status;
+	u64 __padding1[7];
+	u64 complete;
+	u64 __padding2[7];
+};
+
+struct qib_knx_tid_info {
+	/* this is the entire set of 512 entries (= 4K) so
+	 * we can resgister. subctxt devision will be done
+	 * in MIC driver. */
+	off_t tidbase_offset;
+	size_t tidbase_len;
+	u64 tidbase;
+	unsigned tidcnt;
+	u64 tidtemplate;
+	unsigned long invalidtid;
+	u64 bar_addr;
+	u64 bar_len;
+};
+
+#endif /* _QIB_KNX_COMMON_H */
diff -ruN a9/drivers/infiniband/hw/qib/qib_knx.h a10/drivers/infiniband/hw/qib/qib_knx.h
--- a9/drivers/infiniband/hw/qib/qib_knx.h	1969-12-31 16:00:00.000000000 -0800
+++ a10/drivers/infiniband/hw/qib/qib_knx.h	2015-09-10 09:36:03.141932310 -0700
@@ -0,0 +1,74 @@
+/*
+ * Copyright (c) 2012, 2013 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+#ifndef _QIB_KNX_H
+#define _QIB_KNX_H
+
+#include "qib.h"
+
+enum qib_knx_ctxtinfo_type {
+	QIB_KNX_CTXTINFO_UREG,
+	QIB_KNX_CTXTINFO_PIOAVAIL,
+	QIB_KNX_CTXTINFO_STATUS,
+	QIB_KNX_CTXTINFO_PIOBUFBASE,
+	QIB_KNX_CTXTINFO_FLAGS
+};
+
+#ifdef QIB_CONFIG_KNX
+int __init qib_knx_server_init(void);
+void __exit qib_knx_server_exit(void);
+
+void qib_knx_remove_device(struct qib_devdata *);
+
+inline struct qib_knx *qib_knx_get(uint16_t);
+inline struct qib_devdata *qib_knx_node_to_dd(uint16_t);
+int qib_knx_alloc_ctxt(u16, unsigned);
+int qib_knx_setup_piobufs(struct qib_devdata *, struct qib_ctxtdata *, __u16);
+int qib_knx_setup_pioregs(struct qib_devdata *, struct qib_ctxtdata *,
+			  struct qib_base_info *);
+int qib_knx_create_rcvhdrq(struct qib_devdata *, struct qib_ctxtdata *,
+			   struct qib_base_info *);
+int qib_knx_setup_eagerbufs(struct qib_ctxtdata *, struct qib_base_info *);
+void qib_knx_free_ctxtdata(struct qib_devdata *, struct qib_ctxtdata *);
+__u64 qib_knx_ctxt_info(struct qib_ctxtdata *, enum qib_knx_ctxtinfo_type,
+			struct file *);
+int qib_knx_sdma_queue_create(struct file *);
+void qib_knx_sdma_queue_destroy(struct qib_filedata *);
+#else
+static inline u64 qib_knx_ctxt_info(
+	struct qib_ctxtdata *rcd,
+	enum qib_knx_ctxtinfo_type type,
+	struct file *fp)
+{
+	return 0;
+}
+#endif
+#endif /* _QIB_KNX_H */
diff -ruN a9/drivers/infiniband/hw/qib/qib_user_sdma.c a10/drivers/infiniband/hw/qib/qib_user_sdma.c
--- a9/drivers/infiniband/hw/qib/qib_user_sdma.c	2015-09-10 09:35:36.421958157 -0700
+++ a10/drivers/infiniband/hw/qib/qib_user_sdma.c	2015-09-10 09:36:03.141932310 -0700
@@ -63,80 +63,6 @@
 	pid_t pid;
 };
 
-struct qib_user_sdma_pkt {
-	struct list_head list;  /* list element */
-
-	u8  tiddma;		/* if this is NEW tid-sdma */
-	u8  largepkt;		/* this is large pkt from kmalloc */
-	u16 frag_size;		/* frag size used by PSM */
-	u16 index;              /* last header index or push index */
-	u16 naddr;              /* dimension of addr (1..3) ... */
-	u16 addrlimit;		/* addr array size */
-	u16 tidsmidx;		/* current tidsm index */
-	u16 tidsmcount;		/* tidsm array item count */
-	u16 payload_size;	/* payload size so far for header */
-	u32 bytes_togo;		/* bytes for processing */
-	u32 counter;            /* sdma pkts queued counter for this entry */
-	struct qib_tid_session_member *tidsm;	/* tid session member array */
-	struct qib_user_sdma_queue *pq;	/* which pq this pkt belongs to */
-	u64 added;              /* global descq number of entries */
-
-	struct {
-		u16 offset;                     /* offset for kvaddr, addr */
-		u16 length;                     /* length in page */
-		u16 first_desc;			/* first desc */
-		u16 last_desc;			/* last desc */
-		u16 put_page;                   /* should we put_page? */
-		u16 dma_mapped;                 /* is page dma_mapped? */
-		u16 dma_length;			/* for dma_unmap_page() */
-		u16 padding;
-		struct page *page;              /* may be NULL (coherent mem) */
-		void *kvaddr;                   /* FIXME: only for pio hack */
-		dma_addr_t addr;
-	} addr[4];   /* max pages, any more and we coalesce */
-};
-
-struct qib_user_sdma_queue {
-	/*
-	 * pkts sent to dma engine are queued on this
-	 * list head.  the type of the elements of this
-	 * list are struct qib_user_sdma_pkt...
-	 */
-	struct list_head sent;
-
-	/*
-	 * Because above list will be accessed by both process and
-	 * signal handler, we need a spinlock for it.
-	 */
-	spinlock_t sent_lock ____cacheline_aligned_in_smp;
-
-	/* headers with expected length are allocated from here... */
-	char header_cache_name[64];
-	struct dma_pool *header_cache;
-
-	/* packets are allocated from the slab cache... */
-	char pkt_slab_name[64];
-	struct kmem_cache *pkt_slab;
-
-	/* as packets go on the queued queue, they are counted... */
-	u32 counter;
-	u32 sent_counter;
-	/* pending packets, not sending yet */
-	u32 num_pending;
-	/* sending packets, not complete yet */
-	u32 num_sending;
-	/* global descq number of entry of last sending packet */
-	u64 added;
-
-	/* dma page table */
-	struct rb_root dma_pages_root;
-
-	struct qib_user_sdma_rb_node *sdma_rb_node;
-
-	/* protect everything above... */
-	struct mutex lock;
-};
-
 static struct qib_user_sdma_rb_node *
 qib_user_sdma_rb_search(struct rb_root *root, pid_t pid)
 {
@@ -254,12 +180,12 @@
 	return pq;
 }
 
-static void qib_user_sdma_init_frag(struct qib_user_sdma_pkt *pkt,
-				    int i, u16 offset, u16 len,
-				    u16 first_desc, u16 last_desc,
-				    u16 put_page, u16 dma_mapped,
-				    struct page *page, void *kvaddr,
-				    dma_addr_t dma_addr, u16 dma_length)
+void qib_user_sdma_init_frag(struct qib_user_sdma_pkt *pkt,
+			     int i, u16 offset, u16 len,
+			     u16 first_desc, u16 last_desc,
+			     u16 put_page, u16 dma_mapped,
+			     struct page *page, void *kvaddr,
+			     dma_addr_t dma_addr, u16 dma_length)
 {
 	pkt->addr[i].offset = offset;
 	pkt->addr[i].length = len;
@@ -273,7 +199,7 @@
 	pkt->addr[i].dma_length = dma_length;
 }
 
-static void *qib_user_sdma_alloc_header(struct qib_user_sdma_queue *pq,
+void *qib_user_sdma_alloc_header(struct qib_user_sdma_queue *pq,
 				size_t len, dma_addr_t *dma_addr)
 {
 	void *hdr;
@@ -295,11 +221,11 @@
 	return hdr;
 }
 
-static int qib_user_sdma_page_to_frags(const struct qib_devdata *dd,
-				       struct qib_user_sdma_queue *pq,
-				       struct qib_user_sdma_pkt *pkt,
-				       struct page *page, u16 put,
-				       u16 offset, u16 len, void *kvaddr)
+int qib_user_sdma_page_to_frags(const struct qib_devdata *dd,
+				struct qib_user_sdma_queue *pq,
+				struct qib_user_sdma_pkt *pkt,
+				struct page *page, u16 put,
+				u16 offset, u16 len, void *kvaddr)
 {
 	__le16 *pbc16;
 	void *pbcvaddr;
@@ -314,21 +240,27 @@
 	int ret = 0;
 
 	if (dma_mapping_error(&dd->pcidev->dev, dma_addr)) {
-		/*
-		 * dma mapping error, pkt has not managed
-		 * this page yet, return the page here so
-		 * the caller can ignore this page.
-		 */
-		if (put) {
-			put_page(page);
-		} else {
-			/* coalesce case */
-			kunmap(page);
-			__free_page(page);
+#ifdef QIB_CONFIG_KNX
+		if (!pkt->remote) {
+#endif
+			/*
+			 * dma mapping error, pkt has not managed
+			 * this page yet, return the page here so
+			 * the caller can ignore this page.
+			 */
+			if (put) {
+				put_page(page);
+			} else {
+				/* coalesce case */
+				kunmap(page);
+				__free_page(page);
+			}
+			ret = -ENOMEM;
+			goto done;
 		}
-		ret = -ENOMEM;
-		goto done;
+#ifdef QIB_CONFIG_KNX
 	}
+#endif
 	offset = 0;
 	dma_mapped = 1;
 
@@ -630,13 +562,19 @@
 				       pkt->addr[i].dma_length,
 				       DMA_TO_DEVICE);
 
-		if (pkt->addr[i].kvaddr)
-			kunmap(pkt->addr[i].page);
+#ifdef QIB_CONFIG_KNX
+		if (!pkt->remote) {
+#endif
+			if (pkt->addr[i].kvaddr)
+				kunmap(pkt->addr[i].page);
 
-		if (pkt->addr[i].put_page)
-			put_page(pkt->addr[i].page);
-		else
-			__free_page(pkt->addr[i].page);
+			if (pkt->addr[i].put_page)
+				put_page(pkt->addr[i].page);
+			else
+				__free_page(pkt->addr[i].page);
+#ifdef QIB_CONFIG_KNX
+		}
+#endif
 	} else if (pkt->addr[i].kvaddr) {
 		/* for headers */
 		if (pkt->addr[i].dma_mapped) {
@@ -775,9 +713,9 @@
 }
 
 /* free a packet list -- return counter value of last packet */
-static void qib_user_sdma_free_pkt_list(struct device *dev,
-					struct qib_user_sdma_queue *pq,
-					struct list_head *list)
+void qib_user_sdma_free_pkt_list(struct device *dev,
+				 struct qib_user_sdma_queue *pq,
+				 struct list_head *list)
 {
 	struct qib_user_sdma_pkt *pkt, *pkt_next;
 
@@ -787,6 +725,10 @@
 		for (i = 0; i < pkt->naddr; i++)
 			qib_user_sdma_free_pkt_frag(dev, pq, pkt, i);
 
+#ifdef QIB_CONFIG_KNX
+		if (pkt->remote)
+			qib_knx_sdma_free_pkt(pkt);
+#endif
 		if (pkt->largepkt)
 			kfree(pkt);
 		else
@@ -970,6 +912,9 @@
 		pkt->payload_size = 0;
 		pkt->counter = counter;
 		pkt->tiddma = tiddma;
+#ifdef QIB_CONFIG_KNX
+		pkt->remote = 0;
+#endif
 
 		/* setup the first header */
 		qib_user_sdma_init_frag(pkt, 0, /* index */
@@ -1045,8 +990,8 @@
 }
 
 /* try to clean out queue -- needs pq->lock */
-static int qib_user_sdma_queue_clean(struct qib_pportdata *ppd,
-				     struct qib_user_sdma_queue *pq)
+int qib_user_sdma_queue_clean(struct qib_pportdata *ppd,
+			      struct qib_user_sdma_queue *pq)
 {
 	struct qib_devdata *dd = ppd->dd;
 	struct list_head free_list;
@@ -1110,7 +1055,7 @@
 }
 
 /* clean descriptor queue, returns > 0 if some elements cleaned */
-static int qib_user_sdma_hwqueue_clean(struct qib_pportdata *ppd)
+int qib_user_sdma_hwqueue_clean(struct qib_pportdata *ppd)
 {
 	int ret;
 	unsigned long flags;
@@ -1321,9 +1266,9 @@
 }
 
 /* pq->lock must be held, get packets on the wire... */
-static int qib_user_sdma_push_pkts(struct qib_pportdata *ppd,
-				 struct qib_user_sdma_queue *pq,
-				 struct list_head *pktlist, int count)
+int qib_user_sdma_push_pkts(struct qib_pportdata *ppd,
+			    struct qib_user_sdma_queue *pq,
+			    struct list_head *pktlist, int count)
 {
 	unsigned long flags;
 
diff -ruN a9/drivers/infiniband/hw/qib/qib_user_sdma.h a10/drivers/infiniband/hw/qib/qib_user_sdma.h
--- a9/drivers/infiniband/hw/qib/qib_user_sdma.h	2015-09-10 09:35:36.419958299 -0700
+++ a10/drivers/infiniband/hw/qib/qib_user_sdma.h	2015-09-10 09:36:03.141932310 -0700
@@ -31,12 +31,108 @@
  */
 #include <linux/device.h>
 
-struct qib_user_sdma_queue;
+struct qib_user_sdma_pkt {
+	struct list_head list;  /* list element */
+
+	u8  tiddma;		/* if this is NEW tid-sdma */
+	u8  largepkt;		/* this is large pkt from kmalloc */
+	u16 frag_size;		/* frag size used by PSM */
+	u16 index;              /* last header index or push index */
+	u16 naddr;              /* dimension of addr (1..3) ... */
+	u16 addrlimit;		/* addr array size */
+	u16 tidsmidx;		/* current tidsm index */
+	u16 tidsmcount;		/* tidsm array item count */
+	u16 payload_size;	/* payload size so far for header */
+	u32 bytes_togo;		/* bytes for processing */
+	u32 counter;            /* sdma pkts queued counter for this entry */
+	struct qib_tid_session_member *tidsm;	/* tid session member array */
+	struct qib_user_sdma_queue *pq;	/* which pq this pkt belongs to */
+	u64 added;              /* global descq number of entries */
+#ifdef QIB_CONFIG_KNX
+	u64 remote;             /* does the packet originate on the host */
+#endif
+
+	struct {
+		u16 offset;                     /* offset for kvaddr, addr */
+		u16 length;                     /* length in page */
+		u16 first_desc;			/* first desc */
+		u16 last_desc;			/* last desc */
+		u16 put_page;                   /* should we put_page? */
+		u16 dma_mapped;                 /* is page dma_mapped? */
+		u16 dma_length;			/* for dma_unmap_page() */
+		u16 padding;
+		struct page *page;              /* may be NULL (coherent mem) */
+		void *kvaddr;                   /* FIXME: only for pio hack */
+		dma_addr_t addr;
+	} addr[4];   /* max pages, any more and we coalesce */
+};
+
+struct qib_user_sdma_queue {
+	/*
+	 * pkts sent to dma engine are queued on this
+	 * list head.  the type of the elements of this
+	 * list are struct qib_user_sdma_pkt...
+	 */
+	struct list_head sent;
+
+	/*
+	 * Because above list will be accessed by both process and
+	 * signal handler, we need a spinlock for it.
+	 */
+	spinlock_t sent_lock ____cacheline_aligned_in_smp;
+
+	/* headers with expected length are allocated from here... */
+	char header_cache_name[64];
+	struct dma_pool *header_cache;
+
+	/* packets are allocated from the slab cache... */
+	char pkt_slab_name[64];
+	struct kmem_cache *pkt_slab;
+
+	/* as packets go on the queued queue, they are counted... */
+	u32 counter;
+	u32 sent_counter;
+	/* pending packets, not sending yet */
+	u32 num_pending;
+	/* sending packets, not complete yet */
+	u32 num_sending;
+	/* global descq number of entry of last sending packet */
+	u64 added;
+
+	/* dma page table */
+	struct rb_root dma_pages_root;
+
+	struct qib_user_sdma_rb_node *sdma_rb_node;
+
+	/* protect everything above... */
+	struct mutex lock;
+};
 
 struct qib_user_sdma_queue *
 qib_user_sdma_queue_create(struct device *dev, int unit, int port, int sport);
 void qib_user_sdma_queue_destroy(struct qib_user_sdma_queue *pq);
-
+void *qib_user_sdma_alloc_header(struct qib_user_sdma_queue *pq,
+				 size_t len, dma_addr_t *dma_addr);
+void qib_user_sdma_init_frag(struct qib_user_sdma_pkt *pkt,
+			     int i, u16 offset, u16 len,
+			     u16 first_desc, u16 last_desc,
+			     u16 put_page, u16 dma_mapped,
+			     struct page *page, void *kvaddr,
+			     dma_addr_t dma_addr, u16 dma_length);
+int qib_user_sdma_page_to_frags(const struct qib_devdata *dd,
+				struct qib_user_sdma_queue *pq,
+				struct qib_user_sdma_pkt *pkt,
+				struct page *page, u16 put,
+				u16 offset, u16 len, void *kvaddr);
+int qib_user_sdma_hwqueue_clean(struct qib_pportdata *ppd);
+int qib_user_sdma_queue_clean(struct qib_pportdata *ppd,
+			      struct qib_user_sdma_queue *pq);
+void qib_user_sdma_free_pkt_list(struct device *dev,
+				 struct qib_user_sdma_queue *pq,
+				 struct list_head *list);
+int qib_user_sdma_push_pkts(struct qib_pportdata *ppd,
+			    struct qib_user_sdma_queue *pq,
+			    struct list_head *pktlist, int count);
 int qib_user_sdma_writev(struct qib_ctxtdata *pd,
 			 struct qib_user_sdma_queue *pq,
 			 const struct iovec *iov,
@@ -50,3 +146,9 @@
 
 u32 qib_user_sdma_complete_counter(const struct qib_user_sdma_queue *pq);
 u32 qib_user_sdma_inflight_counter(struct qib_user_sdma_queue *pq);
+
+/*
+ * This function prototype somewhat polutes this header file
+ * but I don't want to create a new header file just for it.
+ */
+void qib_knx_sdma_free_pkt(struct qib_user_sdma_pkt *pkt);
